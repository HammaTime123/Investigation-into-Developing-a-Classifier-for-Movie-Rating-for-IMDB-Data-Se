{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a Classifier for Movie Rating for IMDB Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Load Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets loaded successfully!\n",
      "\n",
      "Validating Training dataset:\n",
      "Warning: Missing values detected.\n",
      "language    1\n",
      "dtype: int64\n",
      "No duplicate rows detected.\n",
      "Dataset shape: (3004, 27)\n",
      "Training dataset structure is valid.\n",
      "Basic statistics:\n",
      "               id  num_critic_for_reviews     duration  \\\n",
      "count  3004.00000             3004.000000  3004.000000   \n",
      "mean   1502.50000              165.609188   110.076565   \n",
      "std     867.32443              121.254549    21.910608   \n",
      "min       1.00000                2.000000    37.000000   \n",
      "25%     751.75000               76.000000    96.000000   \n",
      "50%    1502.50000              137.000000   106.000000   \n",
      "75%    2253.25000              223.000000   120.000000   \n",
      "max    3004.00000              813.000000   330.000000   \n",
      "\n",
      "       director_facebook_likes  actor_3_facebook_likes  \\\n",
      "count              3004.000000             3004.000000   \n",
      "mean                778.890146              767.860186   \n",
      "std                2990.741946             1901.991202   \n",
      "min                   0.000000                0.000000   \n",
      "25%                  10.000000              191.000000   \n",
      "50%                  62.500000              433.000000   \n",
      "75%                 234.000000              683.000000   \n",
      "max               23000.000000            23000.000000   \n",
      "\n",
      "       actor_1_facebook_likes         gross  num_voted_users  \\\n",
      "count             3004.000000  3.004000e+03     3.004000e+03   \n",
      "mean              7654.935752  5.187733e+07     1.049017e+05   \n",
      "std              16488.761947  6.840156e+07     1.508573e+05   \n",
      "min                  0.000000  1.620000e+02     9.100000e+01   \n",
      "25%                728.500000  8.579684e+06     1.897825e+04   \n",
      "50%               1000.000000  3.020586e+07     5.387400e+04   \n",
      "75%              12000.000000  6.647359e+07     1.284895e+05   \n",
      "max             640000.000000  6.586723e+08     1.689764e+06   \n",
      "\n",
      "       cast_total_facebook_likes  facenumber_in_poster  num_user_for_reviews  \\\n",
      "count                3004.000000           3004.000000           3004.000000   \n",
      "mean                11391.643808              1.380826            335.922770   \n",
      "std                 20044.723195              2.093417            415.219466   \n",
      "min                     0.000000              0.000000              4.000000   \n",
      "25%                  1873.000000              0.000000            108.000000   \n",
      "50%                  3889.000000              1.000000            208.000000   \n",
      "75%                 15925.250000              2.000000            398.000000   \n",
      "max                656730.000000             43.000000           5060.000000   \n",
      "\n",
      "        title_year  actor_2_facebook_likes  movie_facebook_likes  \\\n",
      "count  3004.000000             3004.000000           3004.000000   \n",
      "mean   2002.782956             1992.843209           8972.179427   \n",
      "std      10.086250             4651.072554          19853.888300   \n",
      "min    1929.000000                0.000000              0.000000   \n",
      "25%    1999.000000              377.750000              0.000000   \n",
      "50%    2004.000000              664.000000            241.000000   \n",
      "75%    2010.000000              970.000000          11000.000000   \n",
      "max    2016.000000           137000.000000         197000.000000   \n",
      "\n",
      "       average_degree_centrality  imdb_score_binned  \n",
      "count                3004.000000        3004.000000  \n",
      "mean                    0.001739           2.250333  \n",
      "std                     0.001183           0.691451  \n",
      "min                     0.000300           0.000000  \n",
      "25%                     0.000825           2.000000  \n",
      "50%                     0.001426           2.000000  \n",
      "75%                     0.002401           3.000000  \n",
      "max                     0.007354           4.000000  \n",
      "\n",
      "Validating Testing dataset:\n",
      "No missing values detected.\n",
      "No duplicate rows detected.\n",
      "Dataset shape: (752, 26)\n",
      "Testing dataset structure is valid.\n",
      "Basic statistics:\n",
      "               id  num_critic_for_reviews    duration  \\\n",
      "count  752.000000              752.000000  752.000000   \n",
      "mean   376.500000              174.445479  110.982713   \n",
      "std    217.227991              131.713383   25.378172   \n",
      "min      1.000000                8.000000   63.000000   \n",
      "25%    188.750000               81.750000   96.000000   \n",
      "50%    376.500000              142.000000  106.000000   \n",
      "75%    564.250000              229.250000  119.000000   \n",
      "max    752.000000              775.000000  325.000000   \n",
      "\n",
      "       director_facebook_likes  actor_3_facebook_likes  \\\n",
      "count               752.000000              752.000000   \n",
      "mean                920.970745              784.938830   \n",
      "std                3359.461455             1864.192618   \n",
      "min                   0.000000                0.000000   \n",
      "25%                  14.000000              203.500000   \n",
      "50%                  66.500000              457.000000   \n",
      "75%                 243.250000              722.250000   \n",
      "max               22000.000000            19000.000000   \n",
      "\n",
      "       actor_1_facebook_likes         gross  num_voted_users  \\\n",
      "count              752.000000  7.520000e+02     7.520000e+02   \n",
      "mean              8136.437500  5.555090e+07     1.095218e+05   \n",
      "std              10812.436334  7.748123e+07     1.567015e+05   \n",
      "min                  2.000000  7.030000e+02     4.800000e+02   \n",
      "25%                817.500000  6.852100e+06     2.314025e+04   \n",
      "50%               2000.000000  2.916831e+07     5.424550e+04   \n",
      "75%              13250.000000  6.947862e+07     1.289892e+05   \n",
      "max             137000.000000  7.605058e+08     1.347461e+06   \n",
      "\n",
      "       cast_total_facebook_likes  facenumber_in_poster  num_user_for_reviews  \\\n",
      "count                 752.000000            752.000000            752.000000   \n",
      "mean                12068.214096              1.363032            340.519947   \n",
      "std                 14875.918668              1.820834            395.124750   \n",
      "min                     2.000000              0.000000              6.000000   \n",
      "25%                  2027.750000              0.000000            116.000000   \n",
      "50%                  4660.000000              1.000000            218.500000   \n",
      "75%                 17513.500000              2.000000            402.000000   \n",
      "max                140268.000000             15.000000           3054.000000   \n",
      "\n",
      "        title_year  actor_2_facebook_likes  movie_facebook_likes  \\\n",
      "count   752.000000              752.000000            752.000000   \n",
      "mean   2003.750000             2137.352394          10878.398936   \n",
      "std       9.018327             4094.280342          26910.692305   \n",
      "min    1927.000000                0.000000              0.000000   \n",
      "25%    1999.000000              417.000000              0.000000   \n",
      "50%    2005.000000              731.000000            172.500000   \n",
      "75%    2010.000000             1000.000000          12000.000000   \n",
      "max    2016.000000            27000.000000         349000.000000   \n",
      "\n",
      "       average_degree_centrality  \n",
      "count                 752.000000  \n",
      "mean                    0.001839  \n",
      "std                     0.001230  \n",
      "min                     0.000300  \n",
      "25%                     0.000825  \n",
      "50%                     0.001501  \n",
      "75%                     0.002551  \n",
      "max                     0.006754  \n",
      "\n",
      "Validation complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # For data manipulation and analysis\n",
    "from collections import Counter  # For counting hashable objects\n",
    "from sklearn.decomposition import PCA  # For dimensionality reduction\n",
    "import numpy as np  # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For visualization\n",
    "from sklearn.feature_selection import mutual_info_classif  # For feature importance estimation\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression model\n",
    "from sklearn.svm import SVC  # Support Vector Classifier for SVM\n",
    "from sklearn.model_selection import KFold  # For k-fold cross-validation\n",
    "import statistics as stat  # For statistical calculations\n",
    "from sklearn.preprocessing import StandardScaler  # For standardizing features\n",
    "from sklearn.cross_decomposition import PLSRegression  # Partial Least Squares regression\n",
    "from sklearn.metrics import r2_score, accuracy_score, classification_report, confusion_matrix,f1_score, balanced_accuracy_score  # For calculating R-squared metric\n",
    "import random  # For generating random numbers\n",
    "from sklearn.model_selection import GridSearchCV  # For hyperparameter optimization\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Loading the datasets for training and testing\n",
    "try:\n",
    "    Test = pd.read_csv('project_data/test_dataset.csv')  # Test dataset\n",
    "    Train = pd.read_csv('project_data/train_dataset.csv')  # Training dataset\n",
    "    print(\"Datasets loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Ensure the file paths are correct.\")\n",
    "    raise\n",
    "except pd.errors.EmptyDataError as e:\n",
    "    print(f\"Error: {e}. Ensure the CSV files are not empty.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while loading datasets: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "\n",
    "# Validating the datasets\n",
    "def validate_dataset(df, name):\n",
    "    print(f\"\\nValidating {name} dataset:\")\n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"Warning: Missing values detected.\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "    else:\n",
    "        print(\"No missing values detected.\")\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    if duplicate_count > 0:\n",
    "        print(f\"Warning: {duplicate_count} duplicate rows found.\")\n",
    "    else:\n",
    "        print(\"No duplicate rows detected.\")\n",
    "    \n",
    "    # Check for feature consistency\n",
    "    print(\"Dataset shape:\", df.shape)\n",
    "    if df.empty:\n",
    "        print(f\"Error: {name} dataset is empty.\")\n",
    "        raise ValueError(f\"{name} dataset is empty.\")\n",
    "    elif len(df.columns) == 0:\n",
    "        print(f\"Error: {name} dataset has no columns.\")\n",
    "        raise ValueError(f\"{name} dataset has no columns.\")\n",
    "    else:\n",
    "        print(f\"{name} dataset structure is valid.\")\n",
    "    \n",
    "    # Display basic statistics for numeric columns\n",
    "    print(\"Basic statistics:\")\n",
    "    print(df.describe())\n",
    "\n",
    "# Validate both Train and Test datasets\n",
    "validate_dataset(Train, \"Training\")\n",
    "validate_dataset(Test, \"Testing\")\n",
    "\n",
    "print(\"\\nValidation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load plot keyword embeddings (Doc2Vec representation)\n",
    "KWEembeddings = np.load(\"project_data/features_doc2vec/features_doc2vec/train_doc2vec_features_plot_keywords.npy\")\n",
    "\n",
    "# Load genre embeddings (Doc2Vec representation)\n",
    "Gembeddings = np.load(\"project_data/features_doc2vec/features_doc2vec/train_doc2vec_features_genre.npy\")\n",
    "\n",
    "# Load title embeddings (FastText representation)\n",
    "Tembeddings = np.load(\"project_data/features_fasttext/features_fasttext/train_fasttext_title_embeddings.npy\")\n",
    "\n",
    "\n",
    "KeyWordEmbeddings = pd.DataFrame(KWEembeddings).add_prefix(\"KWE_\")\n",
    "GenreEmbeddings    = pd.DataFrame(Gembeddings).add_prefix(\"GenEmb_\")\n",
    "TitleEmbeddings    = pd.DataFrame(Tembeddings).add_prefix(\"TitleEmb_\")\n",
    "\n",
    "\n",
    "print(\"Shape of KeyWordEmbeddings:\", KeyWordEmbeddings.shape)\n",
    "print(\"Shape of GenreEmbeddings:\", GenreEmbeddings.shape)\n",
    "print(\"Shape of TitleEmbeddings:\", TitleEmbeddings.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Pre-processing, and exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Analyzing Genre Distribution and Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the \"genres\" column into a list of genres for each movie\n",
    "Train[\"genres\"] = Train[\"genres\"].apply(lambda x: x.split(\"|\"))\n",
    "\n",
    "# Initialize an empty list to store all genre occurrences\n",
    "genreFreqList = []\n",
    "\n",
    "# Flatten the nested list of genres to prepare for frequency calculation\n",
    "for genres in Train[\"genres\"]:\n",
    "    for genre in genres:\n",
    "        genreFreqList.append(genre)\n",
    "\n",
    "# Count the occurrences of each genre using Counter\n",
    "count = Counter(genreFreqList)\n",
    "print(\"Genre Frequency Count:\", count)  # Output the genre frequency count\n",
    "print(\"Number of unique genres:\", len(count))  # Output the number of unique genres\n",
    "\n",
    "# Function to one-hot encode genres for each row\n",
    "def OneHotEncodeGenre(row):\n",
    "    \"\"\"\n",
    "    Updates the row's genre columns with one-hot encoding.\n",
    "    Each genre column value is incremented by 1 if the genre is present in the row.\n",
    "\n",
    "    Parameters:\n",
    "    row (Series): A row of the DataFrame containing a list of genres.\n",
    "\n",
    "    Returns:\n",
    "    Series: Updated row with one-hot encoded genre columns.\n",
    "    \"\"\"\n",
    "    for genre in row[\"genres\"]:  # Iterate over genres in the current row\n",
    "        row[genre] += 1  # Increment the respective genre column\n",
    "    return row\n",
    "\n",
    "# Get the list of unique genres as column labels\n",
    "labels = list(count.keys())\n",
    "\n",
    "# Create a DataFrame with one column per genre, initialized to 0\n",
    "EncodedGenres = pd.DataFrame(0, index=np.arange(len(Train)), columns=labels)\n",
    "\n",
    "# Copy the \"genres\" column from the original Train dataset to preserve genre lists\n",
    "EncodedGenres[\"genres\"] = Train[\"genres\"].copy(deep=True)\n",
    "\n",
    "print(\"Initial one-hot encoded DataFrame (before processing):\")\n",
    "print(EncodedGenres)\n",
    "\n",
    "# Apply the one-hot encoding function to each row of the DataFrame\n",
    "EncodedGenres = EncodedGenres.apply(lambda x: OneHotEncodeGenre(x), axis=1)\n",
    "\n",
    "# Drop the now-unnecessary \"genres\" column as the data has been encoded\n",
    "EncodedGenres = EncodedGenres.drop(columns=[\"genres\"])\n",
    "\n",
    "# Output the final one-hot encoded DataFrame\n",
    "print(\"Final one-hot encoded DataFrame (after processing):\")\n",
    "print(EncodedGenres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Content Rating Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Extract content ratings from the dataset\n",
    "ratings = Train[\"content_rating\"]\n",
    "\n",
    "# Generate a frequency list of ratings for analysis\n",
    "ratingsFreqList = [rating for rating in Train[\"content_rating\"]]\n",
    "\n",
    "# Count the occurrences of each rating\n",
    "count = Counter(ratingsFreqList)\n",
    "print(\"Ratings Frequency Count:\", count)\n",
    "\n",
    "# Define a function to convert content ratings into numerical categories\n",
    "def convertRatings(row):\n",
    "    \"\"\"\n",
    "    Convert content ratings to numerical categories:\n",
    "    1 - G, Approved, GP, Passed\n",
    "    2 - PG, PG-13, or any rating containing '13'\n",
    "    3 - M, Not Rated, Unrated\n",
    "    4 - R\n",
    "    5 - X, NC-17, or similar\n",
    "\n",
    "    Parameters:\n",
    "    row (str): The content rating.\n",
    "\n",
    "    Returns:\n",
    "    int: The numerical category for the rating.\n",
    "    \"\"\"\n",
    "    # Ensure the input is a string to avoid errors\n",
    "    row = str(row)\n",
    "\n",
    "    # Map ratings to categories\n",
    "    if 'pg' in row.lower() or '13' in row.lower(): \n",
    "        return 2\n",
    "    if row.lower() in ['g', 'gp', 'passed']:\n",
    "        return 1\n",
    "    if 'm' in row.lower() or row.lower() in [\"not rated\", \"unrated\"]:\n",
    "        return 3\n",
    "    if row.lower() == \"r\":\n",
    "        return 4\n",
    "    if row.lower() in [\"x\", \"nc-17\"] or \"nc\" in row.lower():\n",
    "        return 5\n",
    "    # Default to category 3 if no specific match\n",
    "    return 3\n",
    "\n",
    "# Apply the conversion function to the ratings\n",
    "ratings = ratings.apply(lambda x: convertRatings(x))\n",
    "\n",
    "# Print the transformed ratings for validation\n",
    "print(\"Transformed Ratings (First 10):\")\n",
    "print(ratings.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Country Mapping and One-Hot Encoding of Continents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country Frequency Count: Counter({'USA': 2382, 'UK': 255, 'France': 86, 'Germany': 61, 'Canada': 46, 'Australia': 32, 'Spain': 21, 'Japan': 12, 'New Zealand': 10, 'China': 9, 'Hong Kong': 9, 'Italy': 8, 'South Korea': 7, 'Denmark': 6, 'Mexico': 6, 'Ireland': 6, 'Brazil': 4, 'India': 3, 'Thailand': 3, 'Norway': 3, 'South Africa': 3, 'Netherlands': 3, 'Argentina': 3, 'Iran': 3, 'Hungary': 2, 'Romania': 2, 'Russia': 2, 'Czech Republic': 2, 'Taiwan': 2, 'Chile': 1, 'Afghanistan': 1, 'Israel': 1, 'Colombia': 1, 'West Germany': 1, 'Official site': 1, 'Aruba': 1, 'Finland': 1, 'Iceland': 1, 'Poland': 1, 'Belgium': 1, 'New Line': 1, 'Indonesia': 1})\n",
      "Continent Frequency Count: Counter({'Americas': 2447, 'Europe': 461, 'Asia': 51, 'Oceania': 42, 'Africa': 3})\n",
      "One-Hot Encoded Continent DataFrame:\n",
      "   Americas  Oceania  Europe  Asia  Africa\n",
      "0         1        0       0     0       0\n",
      "1         1        0       0     0       0\n",
      "2         1        0       0     0       0\n",
      "3         1        0       0     0       0\n",
      "4         0        1       0     0       0\n"
     ]
    }
   ],
   "source": [
    "# Get frequencies of countries in the dataset\n",
    "countryFreqList = [i for i in Train[\"country\"]]\n",
    "country_count = Counter(countryFreqList)\n",
    "print(\"Country Frequency Count:\", country_count)\n",
    "\n",
    "# Prepare a DataFrame to map countries to sub-regions and continents\n",
    "cont = Train[\"country\"]\n",
    "sub_region = pd.DataFrame(\"BLANK\", index=np.arange(len(cont)), columns=[\"sub_region\", \"continent\"])\n",
    "cont = pd.concat([cont, sub_region], axis=1)\n",
    "\n",
    "# Load a country-to-region mapping table\n",
    "countryTable = pd.read_csv(\"project_data/CountryToRegion.csv\")\n",
    "\n",
    "# Function to map countries to their corresponding sub-region and continent\n",
    "def getRegion(row):\n",
    "    \"\"\"\n",
    "    Maps a country to its sub-region and continent based on the provided mapping table.\n",
    "    Defaults to 'Northern America' for sub-region and 'Americas' for continent if no match is found.\n",
    "    \"\"\"\n",
    "    val = countryTable.loc[countryTable[\"country\"] == row[\"country\"]]\n",
    "    if len(val) == 0:\n",
    "        row[\"sub_region\"] = \"Northern America\"  # Default sub-region\n",
    "        row[\"continent\"] = \"Americas\"  # Default continent\n",
    "        return row\n",
    "    # Assign sub-region and continent from the mapping table\n",
    "    row[\"sub_region\"] = val.iloc[0][\"sub_region\"]\n",
    "    row[\"continent\"] = val.iloc[0][\"continent\"]\n",
    "    return row\n",
    "\n",
    "# Apply the region-mapping function to the DataFrame\n",
    "cont = cont.apply(lambda x: getRegion(x), axis=1)\n",
    "\n",
    "# Function for one-hot encoding continents\n",
    "def OneHotEncodeSingleCont(row):\n",
    "    \"\"\"\n",
    "    One-hot encodes a single continent for a given row.\n",
    "    \"\"\"\n",
    "    row[row[\"continent\"]] += 1\n",
    "    return row\n",
    "\n",
    "# Get frequencies of continents\n",
    "continentFreqList = [value for value in cont[\"continent\"]]\n",
    "continent_count = Counter(continentFreqList)\n",
    "print(\"Continent Frequency Count:\", continent_count)\n",
    "\n",
    "# Create a DataFrame for one-hot encoding continents\n",
    "labels = list(continent_count.keys())  # Get unique continent labels\n",
    "EncodedCont = pd.DataFrame(0, index=np.arange(len(Train)), columns=labels)  # Initialize one-hot columns\n",
    "EncodedCont[\"continent\"] = cont[\"continent\"].copy(deep=True)\n",
    "\n",
    "# Apply one-hot encoding to each row\n",
    "EncodedCont = EncodedCont.apply(lambda x: OneHotEncodeSingleCont(x), axis=1)\n",
    "\n",
    "# Drop the original \"continent\" column as it is no longer needed\n",
    "EncodedCont = EncodedCont.drop(columns=[\"continent\"])\n",
    "\n",
    "# Display the one-hot encoded continent DataFrame for verification\n",
    "print(\"One-Hot Encoded Continent DataFrame:\")\n",
    "print(EncodedCont.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5: Combining and Scaling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "EncodedCont = EncodedCont.reindex(Train.index)\n",
    "ratings = ratings.reindex(Train.index)\n",
    "TitleEmbeddings = TitleEmbeddings.reindex(Train.index)\n",
    "KeyWordEmbeddings = KeyWordEmbeddings.reindex(Train.index)\n",
    "GenreEmbeddings = GenreEmbeddings.reindex(Train.index)\n",
    "EncodedGenres = EncodedGenres.reindex(Train.index)\n",
    "\n",
    "numericColumnSet = [\n",
    "    'num_critic_for_reviews',\n",
    "    'duration',\n",
    "    'director_facebook_likes',\n",
    "    'actor_3_facebook_likes',\n",
    "    'actor_1_facebook_likes',\n",
    "    'gross',\n",
    "    'num_voted_users',\n",
    "    'cast_total_facebook_likes',\n",
    "    'facenumber_in_poster',\n",
    "    'num_user_for_reviews',\n",
    "    'title_year',\n",
    "    'actor_2_facebook_likes',\n",
    "    'movie_facebook_likes',\n",
    "    'average_degree_centrality'\n",
    "]\n",
    "\n",
    "\n",
    "# PreTotalData = numeric + continents + ratings\n",
    "PreTotalData = pd.concat([Train[numericColumnSet], EncodedCont, ratings], axis=1)\n",
    "PreTotalData.columns = PreTotalData.columns.astype(str)  # ensure column names are strings\n",
    "\n",
    "# TotalData = numeric + continents + ratings + TitleEmbeddings + KeyWordEmbeddings\n",
    "TotalData = pd.concat(\n",
    "    [Train[numericColumnSet], EncodedCont, ratings, TitleEmbeddings, KeyWordEmbeddings],\n",
    "    axis=1\n",
    ")\n",
    "TotalData.columns = TotalData.columns.astype(str)\n",
    "\n",
    "# TotalDataPlusGenreEmbeddings = TotalData + GenreEmbeddings\n",
    "TotalDataPlusGenreEmbeddings = pd.concat([TotalData, GenreEmbeddings], axis=1)\n",
    "TotalDataPlusGenreEmbeddings.columns = TotalDataPlusGenreEmbeddings.columns.astype(str)\n",
    "\n",
    "# TotalDataPlusGenreEncodings = TotalData + EncodedGenres\n",
    "TotalDataPlusGenreEncodings = pd.concat([TotalData, EncodedGenres], axis=1)\n",
    "TotalDataPlusGenreEncodings.columns = TotalDataPlusGenreEncodings.columns.astype(str)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale PreTotalData\n",
    "S_TotalData_array = scaler.fit_transform(PreTotalData)\n",
    "S_TotalData = pd.DataFrame(S_TotalData_array, columns=PreTotalData.columns)\n",
    "\n",
    "# Scale TotalDataPlusGenreEmbeddings\n",
    "scaled_embeddings_data = scaler.fit_transform(TotalDataPlusGenreEmbeddings)\n",
    "S_TotalDataPlusGenreEmbeddings = pd.DataFrame(\n",
    "    scaled_embeddings_data,\n",
    "    columns=TotalDataPlusGenreEmbeddings.columns\n",
    ")\n",
    "\n",
    "# Scale TotalDataPlusGenreEncodings\n",
    "scaled_encoding_data = scaler.fit_transform(TotalDataPlusGenreEncodings)\n",
    "S_TotalDataPlusGenreEncoding = pd.DataFrame(\n",
    "    scaled_encoding_data,\n",
    "    columns=TotalDataPlusGenreEncodings.columns\n",
    ")\n",
    "\n",
    "# Verify Shapes\n",
    "print(\"Shape of S_TotalData:\", S_TotalData.shape)\n",
    "print(\"Shape of S_TotalDataPlusGenreEmbeddings (scaled):\", S_TotalDataPlusGenreEmbeddings.shape)\n",
    "print(\"Shape of S_TotalDataPlusGenreEncoding (scaled):\", S_TotalDataPlusGenreEncoding.shape)\n",
    "\n",
    "\n",
    "y = Train[\"imdb_score_binned\"]  # the target\n",
    "\n",
    "# Split for S_TotalData\n",
    "X_train_S_TotalData, X_test_S_TotalData, y_train_S_TotalData, y_test_S_TotalData = train_test_split(\n",
    "    S_TotalData,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Split for S_TotalDataPlusGenreEmbeddings\n",
    "X_train_S_TotalDataPlusGenreEmbeddings, X_test_S_TotalDataPlusGenreEmbeddings, y_train_S_TotalDataPlusGenreEmbeddings, y_test_S_TotalDataPlusGenreEmbeddings = train_test_split(\n",
    "    S_TotalDataPlusGenreEmbeddings,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Split for S_TotalDataPlusGenreEncoding\n",
    "X_train_S_TotalDataPlusGenreEncoding, X_test_S_TotalDataPlusGenreEncoding, y_train_S_TotalDataPlusGenreEncoding, y_test_S_TotalDataPlusGenreEncoding = train_test_split(\n",
    "    S_TotalDataPlusGenreEncoding,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"S_TotalData train shape:\", X_train_S_TotalData.shape, \"test shape:\", X_test_S_TotalData.shape)\n",
    "print(\"S_TotalDataPlusGenreEmbeddings train shape:\", X_train_S_TotalDataPlusGenreEmbeddings.shape,\n",
    "      \"test shape:\", X_test_S_TotalDataPlusGenreEmbeddings.shape)\n",
    "print(\"S_TotalDataPlusGenreEncoding train shape:\", X_train_S_TotalDataPlusGenreEncoding.shape,\n",
    "      \"test shape:\", X_test_S_TotalDataPlusGenreEncoding.shape)\n",
    "\n",
    "# Check data types if desired\n",
    "print(\"\\nType of X_train_S_TotalDataPlusGenreEncoding:\", type(X_train_S_TotalDataPlusGenreEncoding))\n",
    "print(\"Type of y_train_S_TotalDataPlusGenreEncoding:\", type(y_train_S_TotalDataPlusGenreEncoding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Feature Engineering and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Principal Component Analysis (PCA) on Keyword Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA on the KeyWordEmbeddings\n",
    "pca = PCA()\n",
    "pca.fit(KeyWordEmbeddings)  # Fit PCA model on the embeddings\n",
    "\n",
    "# Extract explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Set up the plot aesthetics using seaborn\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create the figure and axis objects for cumulative variance plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot cumulative variance with markers\n",
    "ax.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, \n",
    "        marker='o', linestyle='--', label='Cumulative Explained Variance', color='blue')\n",
    "\n",
    "# Highlight the point where 95% variance is explained\n",
    "components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "ax.axhline(y=0.95, color='red', linestyle='-', label='95% Variance Threshold')\n",
    "ax.axvline(x=components_95, color='green', linestyle='--', \n",
    "           label=f'{components_95} Components for 95% Variance')\n",
    "\n",
    "# Add annotations for clarity\n",
    "ax.annotate(f'{components_95} Components', \n",
    "            xy=(components_95, 0.95), \n",
    "            xytext=(components_95 + 5, 0.85),\n",
    "            arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "            fontsize=12)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Explained Variance vs. Number of Principal Components', fontsize=16)\n",
    "ax.set_xlabel('Number of Principal Components', fontsize=14)\n",
    "ax.set_ylabel('Cumulative Explained Variance', fontsize=14)\n",
    "ax.set_xlim(1, len(cumulative_variance))\n",
    "ax.set_ylim(0.0, 1.05)\n",
    "ax.legend(loc='lower right', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap to visualize contribution of principal components\n",
    "contribution_matrix = pca.components_[:components_95]\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(contribution_matrix, cmap='coolwarm', center=0, \n",
    "            annot=False, fmt=\".2f\", xticklabels=False, yticklabels=True)\n",
    "plt.title(f'Contribution of First {components_95} Principal Components', fontsize=16)\n",
    "plt.xlabel('Features', fontsize=14)\n",
    "plt.ylabel('Principal Components', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature Evaluation and PCA on Genre Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate mutual information between title embeddings and binned IMDb scores\n",
    "print(\"Mutual Information between Title Embeddings and IMDb Score Bins:\")\n",
    "mi_scores = mutual_info_classif(TitleEmbeddings, Train[\"imdb_score_binned\"])\n",
    "print(mi_scores)\n",
    "\n",
    "# Calculate mutual information scores\n",
    "mi_scores = mutual_info_classif(TitleEmbeddings, Train[\"imdb_score_binned\"])\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "mi_df = pd.DataFrame({'Feature': [f'Feature_{i+1}' for i in range(len(mi_scores))], 'MI Score': mi_scores})\n",
    "\n",
    "# Sort the DataFrame by MI scores in descending order for better insights\n",
    "mi_df = mi_df.sort_values(by='MI Score', ascending=False)\n",
    "\n",
    "# Plot the mutual information scores as a bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(mi_df['Feature'], mi_df['MI Score'], color='skyblue')\n",
    "\n",
    "# Add plot labels and title\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Mutual Information Score', fontsize=12)\n",
    "plt.title('Mutual Information Scores for Title Embeddings', fontsize=14)\n",
    "\n",
    "# Rotate x-axis labels for better visibility\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "# Perform PCA on GenreEmbeddings\n",
    "pca = PCA()\n",
    "pca.fit(GenreEmbeddings)\n",
    "\n",
    "# Print the explained variance ratio for PCA\n",
    "print(\"Explained Variance Ratio for Genre Embeddings PCA:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot cumulative explained variance for GenreEmbeddings\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)  # Set the figure size\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# x-axis: Component indices (1-based for human readability)\n",
    "xi = np.arange(1, len(pca.explained_variance_ratio_) + 1, step=1)\n",
    "\n",
    "# y-axis: Cumulative sum of explained variance ratios\n",
    "y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.ylim(0.0, 1.1)  # Set y-axis range\n",
    "plt.plot(xi, y, marker='o', linestyle='--', color='b', label='Cumulative Variance')\n",
    "\n",
    "# Add plot labels and title\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(0, len(pca.explained_variance_ratio_) + 1, step=10))  # Adjust step size if needed\n",
    "plt.ylabel('Cumulative Variance (%)')\n",
    "plt.title('The Number of Components Needed to Explain Variance')\n",
    "\n",
    "# Add reference line for the 95% variance threshold\n",
    "plt.axhline(y=0.95, color='r', linestyle='-', label='95% Threshold')\n",
    "plt.text(10, 0.88, '95% cut-off threshold', color='red', fontsize=12)\n",
    "\n",
    "# Add gridlines and legend for clarity\n",
    "ax.grid(axis='x')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Partial Least Squares (PLS) Regression and Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features and target variable\n",
    "X_scaled = S_TotalDataPlusGenreEmbeddings\n",
    "y_scaled = Train[\"imdb_score_binned\"].values.reshape(-1, 1).ravel()\n",
    "\n",
    "# Fit PLS regression with standardized data\n",
    "pls = PLSRegression(n_components=len(S_TotalDataPlusGenreEmbeddings.columns) - 1, max_iter=100000)\n",
    "pls.fit(X_scaled, y_scaled)\n",
    "\n",
    "# Evaluate R² for individual components and overall performance\n",
    "def pls_explained_variance(pls, X, Y_true, do_plot=True):\n",
    "    r2 = np.zeros(pls.n_components)\n",
    "    x_transformed = pls.transform(X)\n",
    "    for i in range(0, pls.n_components):\n",
    "        Y_pred = (np.dot(x_transformed[:, i][:, np.newaxis],\n",
    "                         pls.y_loadings_[:, i][:, np.newaxis].T) * pls._y_std\n",
    "                  + pls._y_mean)\n",
    "        r2[i] = r2_score(Y_true, Y_pred)\n",
    "    \n",
    "    overall_r2 = r2_score(Y_true, pls.predict(X))\n",
    "\n",
    "    if do_plot:\n",
    "        import seaborn as sns\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.lineplot(x=np.arange(1, pls.n_components + 1), y=r2, marker='o', label='Individual R²')\n",
    "        plt.axhline(overall_r2, color='r', linestyle='--', label=f'Overall R²: {overall_r2:.3f}')\n",
    "        plt.xlabel('PLS Component #', fontsize=12)\n",
    "        plt.ylabel('R²', fontsize=12)\n",
    "        plt.title(f'Summed Individual R²: {np.sum(r2):.3f}, Overall R²: {overall_r2:.3f}', fontsize=14)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return r2, overall_r2\n",
    "\n",
    "# Evaluate and plot explained variance\n",
    "r2, overall_r2 = pls_explained_variance(pls, X_scaled, y_scaled, do_plot=True)\n",
    "\n",
    "# Find the optimal number of PLS components using cross-validation\n",
    "best_r2 = float('-inf')\n",
    "best_n_components = 0\n",
    "\n",
    "for n in range(1, 40):  # Test components from 1 to 20\n",
    "    pls = PLSRegression(n_components=n)\n",
    "    scores = cross_val_score(pls, X_scaled, y_scaled, cv=5, scoring='r2')\n",
    "    mean_r2 = np.mean(scores)\n",
    "    # print(f\"Components: {n}, Mean R²: {mean_r2:.3f}\")\n",
    "    if mean_r2 > best_r2:\n",
    "        best_r2 = mean_r2\n",
    "        best_n_components = n\n",
    "\n",
    "print(f\"Best number of components: {best_n_components}, Best R²: {best_r2:.3f}\")\n",
    "\n",
    "# Fit PLS with the optimal number of components\n",
    "pls = PLSRegression(n_components=best_n_components)\n",
    "pls.fit(X_scaled, y_scaled)\n",
    "r2, overall_r2 = pls_explained_variance(pls, X_scaled, y_scaled, do_plot=True)\n",
    "\n",
    "\n",
    "# Calculate cumulative variance for selected components\n",
    "PLScomponents = pls.transform(X_scaled)\n",
    "tempList = []\n",
    "for i in range(1, best_n_components + 1):\n",
    "    Itemp = PLScomponents[:, i - 1]\n",
    "    Jtemp = y_scaled\n",
    "    tempList.append(np.abs(np.cov(Itemp, Jtemp)[1, 0]))\n",
    "\n",
    "# Plot cumulative variance explained by components\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "xi = np.arange(1, best_n_components + 1, step=1)\n",
    "y = np.cumsum(tempList) / sum(tempList)\n",
    "\n",
    "# Calculate cumulative variance for selected components\n",
    "PLScomponents = pls.transform(X_scaled)\n",
    "tempList = []\n",
    "for i in range(1, best_n_components + 1):\n",
    "    Itemp = PLScomponents[:, i - 1]\n",
    "    Jtemp = y_scaled\n",
    "    tempList.append(np.abs(np.cov(Itemp, Jtemp)[1, 0]))\n",
    "\n",
    "# Plot cumulative variance explained by components\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "xi = np.arange(1, best_n_components + 1, step=1)\n",
    "y = np.cumsum(tempList) / sum(tempList)\n",
    "\n",
    "n_comp_95 = np.argmax(y >= 0.95) + 1\n",
    "\n",
    "# Existing plot code\n",
    "sns.lineplot(x=xi, y=y, marker='o', linestyle='--', color='b', label='Cumulative Variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='-', label='95% Cut-off Threshold')\n",
    "\n",
    "# Existing vertical line/text for best_n_components\n",
    "plt.vlines(best_n_components, 0, 1.2, linestyles='dotted', colors='gray', \n",
    "           label=f'Selected Components ({best_n_components})')\n",
    "plt.text(best_n_components + 0.5, 0.5, f'{best_n_components} Best R² Components', \n",
    "         color='black', fontsize=10)\n",
    "\n",
    "plt.vlines(n_comp_95, 0, 1.2, linestyles='--', colors='green',\n",
    "           label=f'95% Variance at {n_comp_95} Components')\n",
    "plt.text(n_comp_95 + 0.5, 0.4, f'{n_comp_95} Components for 95% Var',\n",
    "         color='green', fontsize=10)\n",
    "\n",
    "plt.ylim(0.0, 1.1)\n",
    "plt.xlabel('Number of Components', fontsize=12)\n",
    "plt.ylabel('Cumulative Variance (%)', fontsize=12)\n",
    "plt.title('Cumulative Variance Explained by Selected Components', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Dataset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Baseline Whole Dataset Evaluation with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWrapper:\n",
    "    def __init__(self, name, X_train, y_train, X_test, y_test):\n",
    "        self.name = name\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "# Create your dataset wrappers (replace with your own variables)\n",
    "datasets = [\n",
    "    DatasetWrapper(\n",
    "        \"TotalData\",\n",
    "        X_train_S_TotalData,\n",
    "        y_train_S_TotalData,\n",
    "        X_test_S_TotalData,\n",
    "        y_test_S_TotalData\n",
    "    ),\n",
    "    DatasetWrapper(\n",
    "        \"TotalDataPlusGenreEncodings\",\n",
    "        X_train_S_TotalDataPlusGenreEncoding,\n",
    "        y_train_S_TotalDataPlusGenreEncoding,\n",
    "        X_test_S_TotalDataPlusGenreEncoding,\n",
    "        y_test_S_TotalDataPlusGenreEncoding\n",
    "    ),\n",
    "    DatasetWrapper(\n",
    "        \"TotalDataPlusGenreEmbeddings\",\n",
    "        X_train_S_TotalDataPlusGenreEmbeddings,\n",
    "        y_train_S_TotalDataPlusGenreEmbeddings,\n",
    "        X_test_S_TotalDataPlusGenreEmbeddings,\n",
    "        y_test_S_TotalDataPlusGenreEmbeddings\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define the models to evaluate\n",
    "models = [\n",
    "    (\"Logistic Regression\", LogisticRegression(max_iter=100000)),\n",
    "    (\"Support Vector Classifier (SVC)\", SVC())\n",
    "]\n",
    "\n",
    "\n",
    "# Function to evaluate models on multiple datasets with cross-validation\n",
    "def evaluate_models_on_splits(datasets, models):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models on multiple train-test splits.\n",
    "\n",
    "    Parameters:\n",
    "    - datasets: List of DatasetWrapper objects, each containing X_train, y_train, X_test, y_test\n",
    "    - models: List of (model_name, model_instance) tuples\n",
    "\n",
    "    Prints out the accuracy on the test set for each dataset and each model.\n",
    "    \"\"\"\n",
    "    for ds in datasets:\n",
    "        print(f\"\\nEvaluating Dataset: {ds.name}\")\n",
    "        for model_name, model in models:\n",
    "            # Fit the model on the train set\n",
    "            model.fit(ds.X_train, ds.y_train)\n",
    "            # Predict on the test set\n",
    "            y_pred = model.predict(ds.X_test)\n",
    "            # Calculate test accuracy\n",
    "            accuracy = accuracy_score(ds.y_test, y_pred)\n",
    "            print(f\"  Model: {model_name} | Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_models_on_splits(datasets, models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Selection Using Ablation Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding feature: num_voted_users, new score = 0.38599694752168207\n",
      "Current combo: ['num_voted_users']\n",
      "Adding feature: Drama, new score = 0.4791727422188904\n",
      "Current combo: ['num_voted_users', 'Drama']\n",
      "Adding feature: Americas, new score = 0.5167683544153876\n",
      "Current combo: ['num_voted_users', 'Drama', 'Americas']\n",
      "Adding feature: num_user_for_reviews, new score = 0.5448299675659698\n",
      "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews']\n",
      "Adding feature: title_year, new score = 0.5581982059271221\n",
      "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year']\n",
      "Adding feature: Action, new score = 0.5733545561990545\n",
      "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action']\n",
      "Adding feature: Documentary, new score = 0.5808241809600497\n",
      "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary']\n",
      "Adding feature: Biography, new score = 0.5848410912192158\n",
      "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography']\n",
      "Adding feature: War, new score = 0.5883302454831627\n",
      "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War']\n",
      "Adding feature: TitleEmb_76, new score = 0.5894107227712786\n",
      "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76']\n",
      "Adding feature: TitleEmb_20, new score = 0.5909957473707509\n",
      "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76', 'TitleEmb_20']\n",
      "Adding feature: Fantasy, new score = 0.5914188776188001\n",
      "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76', 'TitleEmb_20', 'Fantasy']\n",
      "Adding feature: Musical, new score = 0.5927054682966036\n",
      "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76', 'TitleEmb_20', 'Fantasy', 'Musical']\n",
      "Adding feature: Europe, new score = 0.5928882165103557\n",
      "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76', 'TitleEmb_20', 'Fantasy', 'Musical', 'Europe']\n",
      "Adding feature: Africa, new score = 0.592923044725544\n",
      "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76', 'TitleEmb_20', 'Fantasy', 'Musical', 'Europe', 'Africa']\n",
      "Adding feature: Film-Noir, new score = 0.5929273169898461\n",
      "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76', 'TitleEmb_20', 'Fantasy', 'Musical', 'Europe', 'Africa', 'Film-Noir']\n",
      "Adding feature: Oceania, new score = 0.5937487568863524\n",
      "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76', 'TitleEmb_20', 'Fantasy', 'Musical', 'Europe', 'Africa', 'Film-Noir', 'Oceania']\n",
      "Ablation with feature set size: 17\n",
      "Ablation baseline score: 0.5937487568863524\n",
      "No improvement after trying to remove 'Oceania' (score 0.5929273169898461 < baseline 0.5937487568863524). Stopping.\n",
      "Final selected features: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76', 'TitleEmb_20', 'Fantasy', 'Musical', 'Europe', 'Africa', 'Film-Noir', 'Oceania']\n"
     ]
    }
   ],
   "source": [
    "def accuracy_SVM(X, Y, Xt, Yt):\n",
    "    \"\"\"\n",
    "    Trains an SVM on (X, Y) and returns accuracy on (Xt, Yt).\n",
    "    \"\"\"\n",
    "    clf = SVC()\n",
    "    clf.fit(X, Y)\n",
    "    return clf.score(Xt, Yt)\n",
    "\n",
    "def accuracy_Logistic(X, Y, Xt, Yt):\n",
    "    \"\"\"\n",
    "    Trains a Logistic Regression on (X, Y) and returns accuracy on (Xt, Yt).\n",
    "    \"\"\"\n",
    "    clf = LogisticRegression(max_iter=100000, random_state=1273213)\n",
    "    clf.fit(X, Y)\n",
    "    return clf.score(Xt, Yt)\n",
    "\n",
    "def f1_weighted_SVM(X, Y, Xt, Yt):\n",
    "    clf = SVC(class_weight='balanced', random_state=1273213, max_iter=100000)\n",
    "    clf.fit(X, Y)\n",
    "    y_pred = clf.predict(Xt)\n",
    "    return f1_score(Yt, y_pred, average='weighted')\n",
    "\n",
    "def f1_weighted_Logistic(X, Y, Xt, Yt):\n",
    "    clf = LogisticRegression(max_iter=100000, random_state=1273213, class_weight='balanced')\n",
    "    clf.fit(X, Y)\n",
    "    y_pred = clf.predict(Xt)\n",
    "    return f1_score(Yt, y_pred, average='weighted')\n",
    "\n",
    "def balanced_acc_SVM(X, Y, Xt, Yt):\n",
    "    clf = SVC(class_weight='balanced', random_state=1273213, max_iter=100000)\n",
    "    clf.fit(X, Y)\n",
    "    y_pred = clf.predict(Xt)\n",
    "    return balanced_accuracy_score(Yt, y_pred)\n",
    "\n",
    "def balanced_acc_Logistic(X, Y, Xt, Yt):\n",
    "    clf = LogisticRegression(max_iter=100000, random_state=1273213, class_weight='balanced')\n",
    "    clf.fit(X, Y)\n",
    "    y_pred = clf.predict(Xt)\n",
    "    return balanced_accuracy_score(Yt, y_pred)\n",
    "\n",
    "def f1_weighted_Ensemble(X, Y, Xt, Yt):\n",
    "    svm = SVC(class_weight='balanced', random_state=1273213, max_iter=100000, probability=True)\n",
    "    logistic = LogisticRegression(max_iter=100000, random_state=1273213, class_weight='balanced')\n",
    "    \n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "        ('svm', svm),\n",
    "        ('logistic', logistic)\n",
    "    ], voting='soft', n_jobs=-1)\n",
    "    \n",
    "    ensemble.fit(X, Y)\n",
    "    y_pred = ensemble.predict(Xt)\n",
    "    return f1_score(Yt, y_pred, average='weighted')\n",
    "\n",
    "def balanced_acc_Ensemble(X, Y, Xt, Yt):\n",
    "    svm = SVC(class_weight='balanced', random_state=1273213, max_iter=100000, probability=True)\n",
    "    logistic = LogisticRegression(max_iter=100000, random_state=1273213, class_weight='balanced')\n",
    "    \n",
    "    ensemble = VotingClassifier(estimators=[\n",
    "        ('svm', svm),\n",
    "        ('logistic', logistic)\n",
    "    ], voting='soft', n_jobs=-1)\n",
    "    \n",
    "    ensemble.fit(X, Y)\n",
    "    y_pred = ensemble.predict(Xt)\n",
    "    return balanced_accuracy_score(Yt, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ablationFeatureSelection(fullDataSet, \n",
    "                             targetSeries, \n",
    "                             scoreFunc, \n",
    "                             MaxOrMin, \n",
    "                             nSplits, \n",
    "                             threshold=False):\n",
    "    \"\"\"\n",
    "    Given a dataset (fullDataSet) and a target (targetSeries),\n",
    "    performs 1-step 'ablation': \n",
    "      1) Compute the baseline score across K-folds \n",
    "      2) For each feature, drop it and measure the new score\n",
    "      3) Return the feature whose removal yields the \"best\" score \n",
    "         (depending on MaxOrMin), along with that score.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=nSplits, random_state=10201331, shuffle=True)\n",
    "\n",
    "    # 1) Compute baseline\n",
    "    baseline_scores = []\n",
    "    for train_index, test_index in kf.split(fullDataSet):\n",
    "        X_train = fullDataSet.iloc[train_index]\n",
    "        Y_train = targetSeries.iloc[train_index]\n",
    "        X_test  = fullDataSet.iloc[test_index]\n",
    "        Y_test  = targetSeries.iloc[test_index]\n",
    "        baseline_scores.append(scoreFunc(X_train, Y_train, X_test, Y_test))\n",
    "    baseline = stat.mean(baseline_scores)\n",
    "    print(\"Ablation baseline score:\", baseline)\n",
    "\n",
    "    # 2) Check the score for removing each feature\n",
    "    feature_scores = {}\n",
    "    for col in fullDataSet.columns:\n",
    "        tmpData = fullDataSet.drop(columns=[col], axis=1)\n",
    "        tmp_scores = []\n",
    "        for train_index, test_index in kf.split(tmpData):\n",
    "            X_train = tmpData.iloc[train_index]\n",
    "            Y_train = targetSeries.iloc[train_index]\n",
    "            X_test  = tmpData.iloc[test_index]\n",
    "            Y_test  = targetSeries.iloc[test_index]\n",
    "            tmp_scores.append(scoreFunc(X_train, Y_train, X_test, Y_test))\n",
    "        feature_scores[col] = stat.mean(tmp_scores)\n",
    "\n",
    "    # 3) Return the column that yields the \"best\" (max or min) score\n",
    "    if MaxOrMin == 'maxamise':\n",
    "        best_val = max(feature_scores.values())\n",
    "        best_cols = [c for c, val in feature_scores.items() if val == best_val]\n",
    "    else:\n",
    "        best_val = min(feature_scores.values())\n",
    "        best_cols = [c for c, val in feature_scores.items() if val == best_val]\n",
    "\n",
    "    chosen_col = random.choice(best_cols)  # pick one if there's a tie\n",
    "    return chosen_col, feature_scores[chosen_col]\n",
    "\n",
    "\n",
    "def buildUp(currFeatureSet, \n",
    "            fullDataSet, \n",
    "            targetSeries, \n",
    "            scoreFunc, \n",
    "            MaxOrMin, \n",
    "            nSplits, \n",
    "            threshold=False):\n",
    "    \"\"\"\n",
    "    Attempt to 'build up' features one-by-one:\n",
    "      - Start from currFeatureSet (possibly empty)\n",
    "      - For each column not yet in currFeatureSet, \n",
    "        add it and compute the cross-validation score\n",
    "      - Return the column that yields the best improvement.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=nSplits, random_state=10201331, shuffle=True)\n",
    "    \n",
    "    # Evaluate each feature *not* in currFeatureSet\n",
    "    candidate_scores = {}\n",
    "    for col in fullDataSet.columns:\n",
    "        if col in currFeatureSet:\n",
    "            continue\n",
    "        combo = currFeatureSet + [col]\n",
    "        tmp_scores = []\n",
    "        for train_index, test_index in kf.split(fullDataSet):\n",
    "            X_train = fullDataSet[combo].iloc[train_index]\n",
    "            Y_train = targetSeries.iloc[train_index]\n",
    "            X_test  = fullDataSet[combo].iloc[test_index]\n",
    "            Y_test  = targetSeries.iloc[test_index]\n",
    "            tmp_scores.append(scoreFunc(X_train, Y_train, X_test, Y_test))\n",
    "        candidate_scores[col] = stat.mean(tmp_scores)\n",
    "\n",
    "    # If there are no candidates left, return None\n",
    "    if len(candidate_scores) == 0:\n",
    "        return None, None\n",
    "\n",
    "    # Identify the best feature to add\n",
    "    if MaxOrMin == 'maxamise':\n",
    "        best_val = max(candidate_scores.values())\n",
    "        best_cols = [c for c, val in candidate_scores.items() if val == best_val]\n",
    "    else:\n",
    "        best_val = min(candidate_scores.values())\n",
    "        best_cols = [c for c, val in candidate_scores.items() if val == best_val]\n",
    "\n",
    "    chosen_col = random.choice(best_cols)\n",
    "    return chosen_col, candidate_scores[chosen_col]\n",
    "\n",
    "random.seed = 10023913 \n",
    "\n",
    "ablationAttempt = S_TotalDataPlusGenreEncoding.copy(deep=True)\n",
    "currFeat = []\n",
    "baseLineAccuracy = 0\n",
    "\n",
    "# BUILD UP STAGE\n",
    "while True:\n",
    "    columnName, accuracy = buildUp(currFeat,\n",
    "                                   ablationAttempt,\n",
    "                                   Train[\"imdb_score_binned\"],\n",
    "                                   f1_weighted_Logistic,\n",
    "                                   'maxamise',\n",
    "                                   5)\n",
    "    if (columnName is None) or (accuracy is None):\n",
    "        # No more features to add\n",
    "        break\n",
    "    if accuracy < baseLineAccuracy:\n",
    "        # Stopping if there's no improvement\n",
    "        break\n",
    "    print(f\"Adding feature: {columnName}, new score = {accuracy}\")\n",
    "    currFeat.append(columnName)\n",
    "    baseLineAccuracy = accuracy\n",
    "    print(\"Current combo:\", currFeat)\n",
    "\n",
    "# ABLATION STAGE\n",
    "baseline = baseLineAccuracy\n",
    "while len(ablationAttempt[currFeat].columns) > 1:\n",
    "    print(\"Ablation with feature set size:\", len(currFeat))\n",
    "    columnName, newScore = ablationFeatureSelection(\n",
    "        ablationAttempt[currFeat], \n",
    "        Train[\"imdb_score_binned\"], \n",
    "        f1_weighted_Logistic,\n",
    "        'maxamise',\n",
    "        5\n",
    "    )\n",
    "    if newScore < baseline:\n",
    "        print(f\"No improvement after trying to remove '{columnName}' (score {newScore} < baseline {baseline}). Stopping.\")\n",
    "        break\n",
    "    print(f\"Removing column: {columnName}, new ablation score = {newScore}\")\n",
    "    currFeat.remove(columnName)\n",
    "    baseline = newScore\n",
    "\n",
    "print(\"Final selected features:\", currFeat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluation of Model on PLS Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=20, random_state=10201331, shuffle=True)\n",
    "\n",
    "avgScr = []\n",
    "for train_index, test_index in kf.split(PLScomponents):\n",
    "    # Slice the first n_comp_95 components for train and test\n",
    "    X = PLScomponents[train_index, :n_comp_95]\n",
    "    Y = Train[\"imdb_score_binned\"].iloc[train_index]\n",
    "    Xt = PLScomponents[test_index, :n_comp_95]\n",
    "    Yt = Train[\"imdb_score_binned\"].iloc[test_index]\n",
    "    \n",
    "    # Fit the Logistic Regression model\n",
    "    clf = LogisticRegression(max_iter=100000)\n",
    "    clf.fit(X, Y)\n",
    "    \n",
    "    # Evaluate on the test fold\n",
    "    avgScr.append(clf.score(Xt, Yt))\n",
    "\n",
    "# Print the mean accuracy across the 20 folds\n",
    "print(\"Mean CV Accuracy:\", stat.mean(avgScr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Model Optimisation and Evaluation with Ablation Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Metrics:\n",
    "F1-Score, Precision-Recall Area Under Curve (PR AUC), Balanced Accuracy, Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' SVM accuracy selection\n",
    " \n",
    "  Adding feature: num_voted_users, new accuracy = 0.6597886855241264\n",
    " Current combo: ['num_voted_users']\n",
    " Adding feature: Drama, new accuracy = 0.6840881863560732\n",
    " Current combo: ['num_voted_users', 'Drama']\n",
    " Adding feature: title_year, new accuracy = 0.7010660011092624\n",
    " Current combo: ['num_voted_users', 'Drama', 'title_year']\n",
    " Adding feature: gross, new accuracy = 0.7120537992235164\n",
    " Current combo: ['num_voted_users', 'Drama', 'title_year', 'gross']\n",
    " Adding feature: TitleEmb_93, new accuracy = 0.7177132556849696\n",
    " Current combo: ['num_voted_users', 'Drama', 'title_year', 'gross', 'TitleEmb_93']\n",
    " Adding feature: Documentary, new accuracy = 0.7200460343871325\n",
    " Current combo: ['num_voted_users', 'Drama', 'title_year', 'gross', 'TitleEmb_93', 'Documentary']\n",
    " Adding feature: Musical, new accuracy = 0.7213782584581254\n",
    " Current combo: ['num_voted_users', 'Drama', 'title_year', 'gross', 'TitleEmb_93', 'Documentary', 'Musical']\n",
    " Ablation with feature set size: 7\n",
    " Ablation baseline score: 0.7213782584581254\n",
    " No improvement after trying to remove 'Musical' (score 0.7200460343871325 < baseline 0.7213782584581254). Stopping.\n",
    " Final selected features: ['num_voted_users', 'Drama', 'title_year', 'gross', 'TitleEmb_93', 'Documentary', 'Musical']\n",
    "'''\n",
    "\n",
    "''' Logistic accuracy selection\n",
    "\n",
    "Adding feature: num_voted_users, new accuracy = 0.6594564614531336\n",
    "Current combo: ['num_voted_users']\n",
    "Adding feature: Drama, new accuracy = 0.6834231835829173\n",
    "Current combo: ['num_voted_users', 'Drama']\n",
    "Adding feature: title_year, new accuracy = 0.6957398779811426\n",
    "Current combo: ['num_voted_users', 'Drama', 'title_year']\n",
    "Adding feature: gross, new accuracy = 0.7113871325568497\n",
    "Current combo: ['num_voted_users', 'Drama', 'title_year', 'gross']\n",
    "Adding feature: Action, new accuracy = 0.7173771491957849\n",
    "Current combo: ['num_voted_users', 'Drama', 'title_year', 'gross', 'Action']\n",
    "Adding feature: Documentary, new accuracy = 0.7213743760399335\n",
    "Current combo: ['num_voted_users', 'Drama', 'title_year', 'gross', 'Action', 'Documentary']\n",
    "Adding feature: KWE_61, new accuracy = 0.7247043815862452\n",
    "Current combo: ['num_voted_users', 'Drama', 'title_year', 'gross', 'Action', 'Documentary', 'KWE_61']\n",
    "Adding feature: Asia, new accuracy = 0.7277021630615641\n",
    "Current combo: ['num_voted_users', 'Drama', 'title_year', 'gross', 'Action', 'Documentary', 'KWE_61', 'Asia']\n",
    "Adding feature: Animation, new accuracy = 0.7313627287853578\n",
    "Current combo: ['num_voted_users', 'Drama', 'title_year', 'gross', 'Action', 'Documentary', 'KWE_61', 'Asia', 'Animation']\n",
    "Adding feature: KWE_57, new accuracy = 0.7343605102606766\n",
    "Current combo: ['num_voted_users', 'Drama', 'title_year', 'gross', 'Action', 'Documentary', 'KWE_61', 'Asia', 'Animation', 'KWE_57']\n",
    "Adding feature: TitleEmb_56, new accuracy = 0.735691625069329\n",
    "Current combo: ['num_voted_users', 'Drama', 'title_year', 'gross', 'Action', 'Documentary', 'KWE_61', 'Asia', 'Animation', 'KWE_57', 'TitleEmb_56']\n",
    "Adding feature: TitleEmb_6, new accuracy = 0.7366894065446479\n",
    "Current combo: ['num_voted_users', 'Drama', 'title_year', 'gross', 'Action', 'Documentary', 'KWE_61', 'Asia', 'Animation', 'KWE_57', 'TitleEmb_56', 'TitleEmb_6']\n",
    "Adding feature: Music, new accuracy = 0.7370227398779812\n",
    "Current combo: ['num_voted_users', 'Drama', 'title_year', 'gross', 'Action', 'Documentary', 'KWE_61', 'Asia', 'Animation', 'KWE_57', 'TitleEmb_56', 'TitleEmb_6', 'Music']\n",
    "Adding feature: Film-Noir, new accuracy = 0.7370232945091514\n",
    "Current combo: ['num_voted_users', 'Drama', 'title_year', 'gross', 'Action', 'Documentary', 'KWE_61', 'Asia', 'Animation', 'KWE_57', 'TitleEmb_56', 'TitleEmb_6', 'Music', 'Film-Noir']\n",
    "Ablation with feature set size: 14\n",
    "Ablation baseline score: 0.7370232945091514\n",
    "No improvement after trying to remove 'Film-Noir' (score 0.7370227398779812 < baseline 0.7370232945091514). Stopping.\n",
    "Final selected features: ['num_voted_users', 'Drama', 'title_year', 'gross', 'Action', 'Documentary', 'KWE_61', 'Asia', 'Animation', 'KWE_57', 'TitleEmb_56', 'TitleEmb_6', 'Music', 'Film-Noir']\n",
    "'''\n",
    "\n",
    "''' Logistic weighted accuracy selection\n",
    "\n",
    "Adding feature: num_voted_users, new accuracy = 0.40937949670755064\n",
    "Current combo: ['num_voted_users']\n",
    "Adding feature: gross, new accuracy = 0.5132430023805099\n",
    "Current combo: ['num_voted_users', 'gross']\n",
    "Adding feature: Drama, new accuracy = 0.5633295299054134\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama']\n",
    "Adding feature: cast_total_facebook_likes, new accuracy = 0.5801622794345427\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes']\n",
    "Adding feature: num_user_for_reviews, new accuracy = 0.5956937018091311\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews']\n",
    "Adding feature: num_critic_for_reviews, new accuracy = 0.6158451571348111\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews']\n",
    "Adding feature: title_year, new accuracy = 0.63122734183215\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews', 'title_year']\n",
    "Adding feature: KWE_14, new accuracy = 0.6481317928130881\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews', 'title_year', 'KWE_14']\n",
    "Adding feature: duration, new accuracy = 0.6547820937240341\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews', 'title_year', 'KWE_14', 'duration']\n",
    "Adding feature: average_degree_centrality, new accuracy = 0.6578447684541728\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews', 'title_year', 'KWE_14', 'duration', 'average_degree_centrality']\n",
    "Adding feature: TitleEmb_24, new accuracy = 0.661872196662387\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews', 'title_year', 'KWE_14', 'duration', 'average_degree_centrality', 'TitleEmb_24']\n",
    "Adding feature: TitleEmb_35, new accuracy = 0.6634514662012416\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews', 'title_year', 'KWE_14', 'duration', 'average_degree_centrality', 'TitleEmb_24', 'TitleEmb_35']\n",
    "Adding feature: TitleEmb_77, new accuracy = 0.6653240192773076\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews', 'title_year', 'KWE_14', 'duration', 'average_degree_centrality', 'TitleEmb_24', 'TitleEmb_35', 'TitleEmb_77']\n",
    "Adding feature: TitleEmb_43, new accuracy = 0.6679770683683393\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews', 'title_year', 'KWE_14', 'duration', 'average_degree_centrality', 'TitleEmb_24', 'TitleEmb_35', 'TitleEmb_77', 'TitleEmb_43']\n",
    "Adding feature: Mystery, new accuracy = 0.6688994335402391\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews', 'title_year', 'KWE_14', 'duration', 'average_degree_centrality', 'TitleEmb_24', 'TitleEmb_35', 'TitleEmb_77', 'TitleEmb_43', 'Mystery']\n",
    "Adding feature: War, new accuracy = 0.669704224611787\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews', 'title_year', 'KWE_14', 'duration', 'average_degree_centrality', 'TitleEmb_24', 'TitleEmb_35', 'TitleEmb_77', 'TitleEmb_43', 'Mystery', 'War']\n",
    "Adding feature: Western, new accuracy = 0.6702203820773973\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews', 'title_year', 'KWE_14', 'duration', 'average_degree_centrality', 'TitleEmb_24', 'TitleEmb_35', 'TitleEmb_77', 'TitleEmb_43', 'Mystery', 'War', 'Western']\n",
    "Adding feature: Africa, new accuracy = 0.6703200578579425\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews', 'title_year', 'KWE_14', 'duration', 'average_degree_centrality', 'TitleEmb_24', 'TitleEmb_35', 'TitleEmb_77', 'TitleEmb_43', 'Mystery', 'War', 'Western', 'Africa']\n",
    "Adding feature: KWE_61, new accuracy = 0.6703450611151022\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews', 'title_year', 'KWE_14', 'duration', 'average_degree_centrality', 'TitleEmb_24', 'TitleEmb_35', 'TitleEmb_77', 'TitleEmb_43', 'Mystery', 'War', 'Western', 'Africa', 'KWE_61']\n",
    "Adding feature: TitleEmb_62, new accuracy = 0.6706294205460057\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews', 'title_year', 'KWE_14', 'duration', 'average_degree_centrality', 'TitleEmb_24', 'TitleEmb_35', 'TitleEmb_77', 'TitleEmb_43', 'Mystery', 'War', 'Western', 'Africa', 'KWE_61', 'TitleEmb_62']\n",
    "Adding feature: KWE_84, new accuracy = 0.6712053213543973\n",
    "Current combo: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews', 'title_year', 'KWE_14', 'duration', 'average_degree_centrality', 'TitleEmb_24', 'TitleEmb_35', 'TitleEmb_77', 'TitleEmb_43', 'Mystery', 'War', 'Western', 'Africa', 'KWE_61', 'TitleEmb_62', 'KWE_84']\n",
    "Ablation with feature set size: 21\n",
    "Ablation baseline score: 0.6712053213543973\n",
    "Removing column: Africa, new ablation score = 0.6713497929317971\n",
    "Ablation with feature set size: 20\n",
    "Ablation baseline score: 0.6713497929317971\n",
    "No improvement after trying to remove 'KWE_84' (score 0.6697033891039782 < baseline 0.6713497929317971). Stopping.\n",
    "Final selected features: ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews', 'title_year', 'KWE_14', 'duration', 'average_degree_centrality', 'TitleEmb_24', 'TitleEmb_35', 'TitleEmb_77', 'TitleEmb_43', 'Mystery', 'War', 'Western', 'KWE_61', 'TitleEmb_62', 'KWE_84']\n",
    "'''\n",
    "\n",
    "''' Logistic f1 selection\n",
    "\n",
    "Adding feature: num_voted_users, new score = 0.38599694752168207\n",
    "Current combo: ['num_voted_users']\n",
    "Adding feature: Drama, new score = 0.4791727422188904\n",
    "Current combo: ['num_voted_users', 'Drama']\n",
    "Adding feature: Americas, new score = 0.5167683544153876\n",
    "Current combo: ['num_voted_users', 'Drama', 'Americas']\n",
    "Adding feature: num_user_for_reviews, new score = 0.5448299675659698\n",
    "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews']\n",
    "Adding feature: title_year, new score = 0.5581982059271221\n",
    "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year']\n",
    "Adding feature: Action, new score = 0.5733545561990545\n",
    "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action']\n",
    "Adding feature: Documentary, new score = 0.5808241809600497\n",
    "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary']\n",
    "Adding feature: Biography, new score = 0.5848410912192158\n",
    "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography']\n",
    "Adding feature: War, new score = 0.5883302454831627\n",
    "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War']\n",
    "Adding feature: TitleEmb_76, new score = 0.5894107227712786\n",
    "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76']\n",
    "Adding feature: TitleEmb_20, new score = 0.5909957473707509\n",
    "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76', 'TitleEmb_20']\n",
    "Adding feature: Fantasy, new score = 0.5914188776188001\n",
    "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76', 'TitleEmb_20', 'Fantasy']\n",
    "Adding feature: Musical, new score = 0.5927054682966036\n",
    "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76', 'TitleEmb_20', 'Fantasy', 'Musical']\n",
    "Adding feature: Europe, new score = 0.5928882165103557\n",
    "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76', 'TitleEmb_20', 'Fantasy', 'Musical', 'Europe']\n",
    "Adding feature: Africa, new score = 0.592923044725544\n",
    "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76', 'TitleEmb_20', 'Fantasy', 'Musical', 'Europe', 'Africa']\n",
    "Adding feature: Film-Noir, new score = 0.5929273169898461\n",
    "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76', 'TitleEmb_20', 'Fantasy', 'Musical', 'Europe', 'Africa', 'Film-Noir']\n",
    "Adding feature: Oceania, new score = 0.5937487568863524\n",
    "Current combo: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76', 'TitleEmb_20', 'Fantasy', 'Musical', 'Europe', 'Africa', 'Film-Noir', 'Oceania']\n",
    "Ablation with feature set size: 17\n",
    "Ablation baseline score: 0.5937487568863524\n",
    "No improvement after trying to remove 'Oceania' (score 0.5929273169898461 < baseline 0.5937487568863524). Stopping.\n",
    "Final selected features: ['num_voted_users', 'Drama', 'Americas', 'num_user_for_reviews', 'title_year', 'Action', 'Documentary', 'Biography', 'War', 'TitleEmb_76', 'TitleEmb_20', 'Fantasy', 'Musical', 'Europe', 'Africa', 'Film-Noir', 'Oceania']\n",
    "'''\n",
    "\n",
    "\n",
    "final_features = ['num_voted_users', 'gross', 'Drama', 'cast_total_facebook_likes', 'num_user_for_reviews', 'num_critic_for_reviews', 'title_year', 'KWE_14', 'duration', 'average_degree_centrality', 'TitleEmb_24', 'TitleEmb_35', 'TitleEmb_77', 'TitleEmb_43', 'Mystery', 'War', 'Western', 'KWE_61', 'TitleEmb_62', 'KWE_84']\n",
    "# ['num_voted_users', 'Drama', 'title_year', 'gross', 'Action', 'Documentary', 'KWE_61', 'Asia', 'Animation', 'KWE_57', 'Musical', 'TitleEmb_10', 'KWE_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Set Up Model Performance Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file 'cumulativeModelData.json' already exists. No changes made.\n",
      "JSON file 'AverageModelData.json' already exists. No changes made.\n"
     ]
    }
   ],
   "source": [
    "# File name\n",
    "file_name = \"cumulativeModelData.json\"\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(file_name):\n",
    "    # Initialize an empty JSON structure\n",
    "    data = {}\n",
    "\n",
    "    # Create the JSON file\n",
    "    with open(file_name, mode=\"w\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    print(f\"JSON file '{file_name}' initialized in the current directory.\")\n",
    "else:\n",
    "    print(f\"JSON file '{file_name}' already exists. No changes made.\")\n",
    "\n",
    "# File name\n",
    "file_name = \"AverageModelData.json\"\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(file_name):\n",
    "    # Initialize an empty JSON structure\n",
    "    data = {\n",
    "\n",
    "    }\n",
    "\n",
    "    # Create the JSON file\n",
    "    with open(file_name, mode=\"w\") as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    print(f\"JSON file '{file_name}' initialized in the current directory.\")\n",
    "else:\n",
    "    print(f\"JSON file '{file_name}' already exists. No changes made.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file 'AverageModelData.json' initialized in the current directory.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'C': 100, 'class_weight': None, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best CV balanced_accuracy: 0.41877697653258117\n",
      "\n",
      "Test balanced_accuracy: 0.4144945574702222\n",
      "\n",
      "Test accuracy: 0.7104825291181365\n",
      "\n",
      "Test f1: 0.6668181609012503\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         5\n",
      "           1       0.25      0.02      0.04        47\n",
      "           2       0.72      0.93      0.81       368\n",
      "           3       0.68      0.43      0.53       155\n",
      "           4       0.78      0.69      0.73        26\n",
      "\n",
      "    accuracy                           0.71       601\n",
      "   macro avg       0.49      0.41      0.42       601\n",
      "weighted avg       0.67      0.71      0.67       601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = \"Logistic_Regression\"\n",
    "\n",
    "X_train = X_train_S_TotalDataPlusGenreEncoding[final_features]\n",
    "y_train = y_train_S_TotalDataPlusGenreEncoding\n",
    "\n",
    "X_test = X_test_S_TotalDataPlusGenreEncoding[final_features]\n",
    "y_test = y_test_S_TotalDataPlusGenreEncoding\n",
    "\n",
    "# parameter grid\n",
    "param_grid = [\n",
    "    {\n",
    "        'penalty': ['l1'],\n",
    "        'solver': ['liblinear', 'saga'],  # only solvers that support l1\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'class_weight': [None]\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['liblinear', 'lbfgs', 'sag', 'saga'],  # solvers that support l2\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'class_weight': [None]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(random_state=1273213, max_iter=100000)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_grid=param_grid,\n",
    "    scoring='balanced_accuracy',   \n",
    "    cv=5,                 # 5-fold cross-validation\n",
    "    n_jobs=-1,            # use all available cores\n",
    "    verbose=1            \n",
    ")\n",
    "\n",
    "# Fit the grid search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearch\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV balanced_accuracy:\", grid_search.best_score_)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_  # best estimator from grid search\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"\\nTest balanced_accuracy:\", test_balanced_acc)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nTest accuracy:\", test_acc)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"\\nTest f1:\", test_f1)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # set figure size\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\", fontsize=16)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "\n",
    "# Adjust tick label size (optional)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 SVC (SVM classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"SVM\"\n",
    "\n",
    "# Prepare the data\n",
    "X_train = X_train_S_TotalDataPlusGenreEncoding[final_features]\n",
    "y_train = y_train_S_TotalDataPlusGenreEncoding\n",
    "\n",
    "X_test = X_test_S_TotalDataPlusGenreEncoding[final_features]\n",
    "y_test = y_test_S_TotalDataPlusGenreEncoding\n",
    "\n",
    "# Define the parameter grid for SVM\n",
    "param_grid = [\n",
    "    {\n",
    "        'kernel': ['linear'],\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['rbf'],\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    },\n",
    "    {\n",
    "        'kernel': ['poly'],\n",
    "        'degree': [2, 3, 4],\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_clf = SVC(random_state=1273213, max_iter=100000)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm_clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='balanced_accuracy',   \n",
    "    cv=5,                 # 5-fold cross-validation\n",
    "    n_jobs=-1,            # Use all available cores\n",
    "    verbose=1          \n",
    ")\n",
    "\n",
    "# Fit the grid search on the training data\n",
    "print(\"Starting Grid Search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Grid Search Completed.\")\n",
    "\n",
    "# Print the best parameters found by GridSearch\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV balanced_accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Retrieve the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate Test Accuracy\n",
    "# Calculate test accuracy\n",
    "test_balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"\\nTest balanced_accuracy:\", test_balanced_acc)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nTest accuracy:\", test_acc)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "print(\"\\nTest f1:\", test_f1)\n",
    "\n",
    "# Generate the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot the Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title(\"Confusion Matrix\", fontsize=16)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Print Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example param grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],      # Number of trees\n",
    "    'max_depth': [None, 5, 10],          # Tree depth\n",
    "    'max_features': ['sqrt', 'log2'],    # Features to consider at each split\n",
    "    'min_samples_split': [2, 5],         # Min # of samples to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],       # Min # of samples at a leaf node\n",
    "    'class_weight': [None, 'balanced'],  # Class imbalance handling\n",
    "}\n",
    "\n",
    "# Create a RandomForestClassifier (baseline model)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Set up the grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='balanced_accuracy',   # or 'f1_macro', etc.\n",
    "    cv=5,                 # 5-fold cross-validation\n",
    "    n_jobs=-1,            # use all available CPU cores\n",
    "    verbose=1             # print progress messages\n",
    ")\n",
    "\n",
    "# Fit on the training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print out the best hyperparameters from the grid search\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best CV balanced_accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Retrieve the best model from the grid search\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "# Calculate test accuracy\n",
    "test_balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"\\nTest balanced_accuracy:\", test_balanced_acc)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nTest accuracy:\", test_acc)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "print(\"\\nTest f1:\", test_f1)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # set figure size\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\", fontsize=16)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "\n",
    "# Adjust tick label size (optional)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators':      [100, 200, 300],\n",
    "    'learning_rate':     [0.01, 0.1, 0.2],\n",
    "    'max_depth':         [3, 5, 7],\n",
    "    'subsample':         [0.8, 1.0],\n",
    "    'colsample_bytree':  [0.8, 1.0],\n",
    "    'min_child_weight':  [1, 3],\n",
    "    'gamma':             [0, 1],\n",
    "    'scale_pos_weight':  [1, 5, 10]\n",
    "}\n",
    "\n",
    "\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',  \n",
    "    eval_metric='logloss',        \n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='balanced_accuracy',    \n",
    "    cv=5,                  \n",
    "    n_jobs=-1,         \n",
    "    verbose=1             \n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best hyperparameters and CV score\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best CV score:\", grid_search.best_score_)\n",
    "\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"\\nTest balanced_accuracy:\", test_balanced_acc)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nTest accuracy:\", test_acc)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_f1 = f1_score(y_test, y_pred)\n",
    "print(\"\\nTest f1:\", test_f1)\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"XGBoost - Confusion Matrix\", fontsize=16)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as dataUtils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, nInputs, l1, l2, l3, nClasses):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.Numerical = torch.nn.Sequential(\n",
    "            torch.nn.Linear(20, 10),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(10,10),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.Embeddings1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(100, 20),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(20,10),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.Embeddings2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(100,20),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(20,10),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.Embeddings3 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(100,20),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(20,10),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.Combination = torch.nn.Sequential(\n",
    "            torch.nn.Linear(40,10),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Dropout(p=0.25),\n",
    "            torch.nn.Linear(10,nClasses),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        a = self.Numerical(x[:,0:20])\n",
    "        b = self.Embeddings1(x[:,20:120])\n",
    "        c = self.Embeddings2(x[:,120:220])\n",
    "        d = self.Embeddings3(x[:,220:320])\n",
    "        combined = torch.cat((a,b,c,d), dim=1)\n",
    "        out = self.Combination(combined)\n",
    "        return out\n",
    "\n",
    "def test(model, criterion, test_loader):\n",
    "    test_loss = 0.\n",
    "    test_preds, test_labels = list(), list()\n",
    "    for i, data in enumerate(test_loader):\n",
    "        x, labels = data\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            test_loss += criterion(input=logits, target=labels).item()\n",
    "            test_preds.append(predictions)\n",
    "            test_labels.append(labels)\n",
    "\n",
    "    test_preds = torch.cat(test_preds)\n",
    "    test_labels = torch.cat(test_labels)\n",
    "\n",
    "    test_accuracy = torch.eq(test_preds, test_labels).float().mean().item()\n",
    "\n",
    "    print('[TEST] Mean loss {:.4f} | Accuracy {:.4f}'.format(test_loss/len(test_loader), test_accuracy))\n",
    "\n",
    "\n",
    "def train(model, train_loader, test_loader, optimizer, n_epochs=10):\n",
    "    LOG_INTERVAL = 250\n",
    "    running_loss, running_accuracy = list(), list()\n",
    "    start_time = time.time()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        epoch_loss = 0.\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "\n",
    "            x, labels = data\n",
    "\n",
    "            logits = model(x)\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            train_acc = torch.mean(torch.eq(predictions, labels).float()).item()\n",
    "\n",
    "            loss = criterion(input=logits, target=labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "            running_accuracy.append(train_acc)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if i % LOG_INTERVAL == 0:\n",
    "                deltaT = time.time() - start_time\n",
    "                mean_loss = epoch_loss / (i+1)\n",
    "                print('[TRAIN] Epoch {} [{}/{}]| Mean loss {:.4f} | Train accuracy {:.5f} | Time {:.2f} s'.format(epoch, \n",
    "                    i, len(train_loader), mean_loss, train_acc, deltaT))\n",
    "\n",
    "        print('Epoch complete! Mean loss: {:.4f}'.format(epoch_loss/len(train_loader)))\n",
    "\n",
    "        test(model, criterion, test_loader)\n",
    "    return running_loss, running_accuracy\n",
    "\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(TotalDataPlusGenreEmbeddings, Train[\"imdb_score_binned\"], test_size=0.2, random_state=981488)\n",
    "\n",
    "xTrain = torch.tensor(xTrain.values, dtype=torch.float)\n",
    "yTrain = torch.tensor(yTrain.values).type(torch.LongTensor)\n",
    "trainData = dataUtils.TensorDataset(xTrain, yTrain)\n",
    "trainLoader = dataUtils.DataLoader(trainData, batch_size=128, shuffle=True)\n",
    "\n",
    "xTest = torch.tensor(xTest.values, dtype=torch.float)\n",
    "yTest = torch.tensor(yTest.values).type(torch.LongTensor)\n",
    "testData = dataUtils.TensorDataset(xTest, yTest)\n",
    "testLoader = dataUtils.DataLoader(testData, batch_size=128, shuffle=False)\n",
    "\n",
    "neuralNetwork = NeuralNetwork(len(TotalDataPlusGenreEmbeddings.columns), 150, 100, 50, 5)\n",
    "\n",
    "optimizer = torch.optim.SGD(neuralNetwork.parameters(), lr=1e-2, momentum=0.5)\n",
    "\n",
    "nnLoss, nnAcc = train(neuralNetwork, trainLoader, testLoader, optimizer, 100)\n",
    "\n",
    "plt.plot(nnLoss)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Cross-entropy Loss (Train)\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(nnAcc)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Accuracy (Train)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
