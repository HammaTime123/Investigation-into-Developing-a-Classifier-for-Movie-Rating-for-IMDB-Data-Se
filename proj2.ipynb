{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a Classifier for Movie Rating for IMDB Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Load Dependencies and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # For data manipulation and analysis\n",
    "from collections import Counter  # For counting hashable objects\n",
    "from sklearn.decomposition import PCA  # For dimensionality reduction\n",
    "import numpy as np  # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For visualization\n",
    "from sklearn.feature_selection import mutual_info_classif  # For feature importance estimation\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression model\n",
    "from sklearn.svm import SVC  # Support Vector Classifier for SVM\n",
    "from sklearn.model_selection import KFold  # For k-fold cross-validation\n",
    "import statistics as stat  # For statistical calculations\n",
    "from sklearn.preprocessing import StandardScaler  # For standardizing features\n",
    "from sklearn.cross_decomposition import PLSRegression  # Partial Least Squares regression\n",
    "from sklearn.metrics import r2_score, accuracy_score, classification_report, confusion_matrix  # For calculating R-squared metric\n",
    "import random  # For generating random numbers\n",
    "from sklearn.model_selection import GridSearchCV  # For hyperparameter optimization\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Loading the datasets for training and testing\n",
    "try:\n",
    "    Test = pd.read_csv('project_data/test_dataset.csv')  # Test dataset\n",
    "    Train = pd.read_csv('project_data/train_dataset.csv')  # Training dataset\n",
    "    print(\"Datasets loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Ensure the file paths are correct.\")\n",
    "    raise\n",
    "except pd.errors.EmptyDataError as e:\n",
    "    print(f\"Error: {e}. Ensure the CSV files are not empty.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while loading datasets: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "\n",
    "# Validating the datasets\n",
    "def validate_dataset(df, name):\n",
    "    print(f\"\\nValidating {name} dataset:\")\n",
    "    # Check for missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"Warning: Missing values detected.\")\n",
    "        print(missing_values[missing_values > 0])\n",
    "    else:\n",
    "        print(\"No missing values detected.\")\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    if duplicate_count > 0:\n",
    "        print(f\"Warning: {duplicate_count} duplicate rows found.\")\n",
    "    else:\n",
    "        print(\"No duplicate rows detected.\")\n",
    "    \n",
    "    # Check for feature consistency\n",
    "    print(\"Dataset shape:\", df.shape)\n",
    "    if df.empty:\n",
    "        print(f\"Error: {name} dataset is empty.\")\n",
    "        raise ValueError(f\"{name} dataset is empty.\")\n",
    "    elif len(df.columns) == 0:\n",
    "        print(f\"Error: {name} dataset has no columns.\")\n",
    "        raise ValueError(f\"{name} dataset has no columns.\")\n",
    "    else:\n",
    "        print(f\"{name} dataset structure is valid.\")\n",
    "    \n",
    "    # Display basic statistics for numeric columns\n",
    "    print(\"Basic statistics:\")\n",
    "    print(df.describe())\n",
    "\n",
    "# Validate both Train and Test datasets\n",
    "validate_dataset(Train, \"Training\")\n",
    "validate_dataset(Test, \"Testing\")\n",
    "\n",
    "print(\"\\nValidation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load plot keyword embeddings (Doc2Vec representation)\n",
    "KWEembeddings = np.load(\"project_data/features_doc2vec/features_doc2vec/train_doc2vec_features_plot_keywords.npy\")\n",
    "\n",
    "# Load genre embeddings (Doc2Vec representation)\n",
    "Gembeddings = np.load(\"project_data/features_doc2vec/features_doc2vec/train_doc2vec_features_genre.npy\")\n",
    "\n",
    "# Load title embeddings (FastText representation)\n",
    "Tembeddings = np.load(\"project_data/features_fasttext/features_fasttext/train_fasttext_title_embeddings.npy\")\n",
    "\n",
    "\n",
    "KeyWordEmbeddings = pd.DataFrame(KWEembeddings).add_prefix(\"KWE_\")\n",
    "GenreEmbeddings    = pd.DataFrame(Gembeddings).add_prefix(\"GenEmb_\")\n",
    "TitleEmbeddings    = pd.DataFrame(Tembeddings).add_prefix(\"TitleEmb_\")\n",
    "\n",
    "\n",
    "print(\"Shape of KeyWordEmbeddings:\", KeyWordEmbeddings.shape)\n",
    "print(\"Shape of GenreEmbeddings:\", GenreEmbeddings.shape)\n",
    "print(\"Shape of TitleEmbeddings:\", TitleEmbeddings.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Pre-processing, and exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Analyzing Genre Distribution and Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the \"genres\" column into a list of genres for each movie\n",
    "Train[\"genres\"] = Train[\"genres\"].apply(lambda x: x.split(\"|\"))\n",
    "\n",
    "# Initialize an empty list to store all genre occurrences\n",
    "genreFreqList = []\n",
    "\n",
    "# Flatten the nested list of genres to prepare for frequency calculation\n",
    "for genres in Train[\"genres\"]:\n",
    "    for genre in genres:\n",
    "        genreFreqList.append(genre)\n",
    "\n",
    "# Count the occurrences of each genre using Counter\n",
    "count = Counter(genreFreqList)\n",
    "print(\"Genre Frequency Count:\", count)  # Output the genre frequency count\n",
    "print(\"Number of unique genres:\", len(count))  # Output the number of unique genres\n",
    "\n",
    "# Function to one-hot encode genres for each row\n",
    "def OneHotEncodeGenre(row):\n",
    "    \"\"\"\n",
    "    Updates the row's genre columns with one-hot encoding.\n",
    "    Each genre column value is incremented by 1 if the genre is present in the row.\n",
    "\n",
    "    Parameters:\n",
    "    row (Series): A row of the DataFrame containing a list of genres.\n",
    "\n",
    "    Returns:\n",
    "    Series: Updated row with one-hot encoded genre columns.\n",
    "    \"\"\"\n",
    "    for genre in row[\"genres\"]:  # Iterate over genres in the current row\n",
    "        row[genre] += 1  # Increment the respective genre column\n",
    "    return row\n",
    "\n",
    "# Get the list of unique genres as column labels\n",
    "labels = list(count.keys())\n",
    "\n",
    "# Create a DataFrame with one column per genre, initialized to 0\n",
    "EncodedGenres = pd.DataFrame(0, index=np.arange(len(Train)), columns=labels)\n",
    "\n",
    "# Copy the \"genres\" column from the original Train dataset to preserve genre lists\n",
    "EncodedGenres[\"genres\"] = Train[\"genres\"].copy(deep=True)\n",
    "\n",
    "print(\"Initial one-hot encoded DataFrame (before processing):\")\n",
    "print(EncodedGenres)\n",
    "\n",
    "# Apply the one-hot encoding function to each row of the DataFrame\n",
    "EncodedGenres = EncodedGenres.apply(lambda x: OneHotEncodeGenre(x), axis=1)\n",
    "\n",
    "# Drop the now-unnecessary \"genres\" column as the data has been encoded\n",
    "EncodedGenres = EncodedGenres.drop(columns=[\"genres\"])\n",
    "\n",
    "# Output the final one-hot encoded DataFrame\n",
    "print(\"Final one-hot encoded DataFrame (after processing):\")\n",
    "print(EncodedGenres)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Content Rating Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Extract content ratings from the dataset\n",
    "ratings = Train[\"content_rating\"]\n",
    "\n",
    "# Generate a frequency list of ratings for analysis\n",
    "ratingsFreqList = [rating for rating in Train[\"content_rating\"]]\n",
    "\n",
    "# Count the occurrences of each rating\n",
    "count = Counter(ratingsFreqList)\n",
    "print(\"Ratings Frequency Count:\", count)\n",
    "\n",
    "# Define a function to convert content ratings into numerical categories\n",
    "def convertRatings(row):\n",
    "    \"\"\"\n",
    "    Convert content ratings to numerical categories:\n",
    "    1 - G, Approved, GP, Passed\n",
    "    2 - PG, PG-13, or any rating containing '13'\n",
    "    3 - M, Not Rated, Unrated\n",
    "    4 - R\n",
    "    5 - X, NC-17, or similar\n",
    "\n",
    "    Parameters:\n",
    "    row (str): The content rating.\n",
    "\n",
    "    Returns:\n",
    "    int: The numerical category for the rating.\n",
    "    \"\"\"\n",
    "    # Ensure the input is a string to avoid errors\n",
    "    row = str(row)\n",
    "\n",
    "    # Map ratings to categories\n",
    "    if 'pg' in row.lower() or '13' in row.lower(): \n",
    "        return 2\n",
    "    if row.lower() in ['g', 'gp', 'passed']:\n",
    "        return 1\n",
    "    if 'm' in row.lower() or row.lower() in [\"not rated\", \"unrated\"]:\n",
    "        return 3\n",
    "    if row.lower() == \"r\":\n",
    "        return 4\n",
    "    if row.lower() in [\"x\", \"nc-17\"] or \"nc\" in row.lower():\n",
    "        return 5\n",
    "    # Default to category 3 if no specific match\n",
    "    return 3\n",
    "\n",
    "# Apply the conversion function to the ratings\n",
    "ratings = ratings.apply(lambda x: convertRatings(x))\n",
    "\n",
    "# Print the transformed ratings for validation\n",
    "print(\"Transformed Ratings (First 10):\")\n",
    "print(ratings.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Country Mapping and One-Hot Encoding of Continents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frequencies of countries in the dataset\n",
    "countryFreqList = [i for i in Train[\"country\"]]\n",
    "country_count = Counter(countryFreqList)\n",
    "print(\"Country Frequency Count:\", country_count)\n",
    "\n",
    "# Prepare a DataFrame to map countries to sub-regions and continents\n",
    "cont = Train[\"country\"]\n",
    "sub_region = pd.DataFrame(\"BLANK\", index=np.arange(len(cont)), columns=[\"sub_region\", \"continent\"])\n",
    "cont = pd.concat([cont, sub_region], axis=1)\n",
    "\n",
    "# Load a country-to-region mapping table\n",
    "countryTable = pd.read_csv(\"CountryToRegion.csv\")\n",
    "\n",
    "# Function to map countries to their corresponding sub-region and continent\n",
    "def getRegion(row):\n",
    "    \"\"\"\n",
    "    Maps a country to its sub-region and continent based on the provided mapping table.\n",
    "    Defaults to 'Northern America' for sub-region and 'Americas' for continent if no match is found.\n",
    "    \"\"\"\n",
    "    val = countryTable.loc[countryTable[\"country\"] == row[\"country\"]]\n",
    "    if len(val) == 0:\n",
    "        row[\"sub_region\"] = \"Northern America\"  # Default sub-region\n",
    "        row[\"continent\"] = \"Americas\"  # Default continent\n",
    "        return row\n",
    "    # Assign sub-region and continent from the mapping table\n",
    "    row[\"sub_region\"] = val.iloc[0][\"sub_region\"]\n",
    "    row[\"continent\"] = val.iloc[0][\"continent\"]\n",
    "    return row\n",
    "\n",
    "# Apply the region-mapping function to the DataFrame\n",
    "cont = cont.apply(lambda x: getRegion(x), axis=1)\n",
    "\n",
    "# Function for one-hot encoding continents\n",
    "def OneHotEncodeSingleCont(row):\n",
    "    \"\"\"\n",
    "    One-hot encodes a single continent for a given row.\n",
    "    \"\"\"\n",
    "    row[row[\"continent\"]] += 1\n",
    "    return row\n",
    "\n",
    "# Get frequencies of continents\n",
    "continentFreqList = [value for value in cont[\"continent\"]]\n",
    "continent_count = Counter(continentFreqList)\n",
    "print(\"Continent Frequency Count:\", continent_count)\n",
    "\n",
    "# Create a DataFrame for one-hot encoding continents\n",
    "labels = list(continent_count.keys())  # Get unique continent labels\n",
    "EncodedCont = pd.DataFrame(0, index=np.arange(len(Train)), columns=labels)  # Initialize one-hot columns\n",
    "EncodedCont[\"continent\"] = cont[\"continent\"].copy(deep=True)\n",
    "\n",
    "# Apply one-hot encoding to each row\n",
    "EncodedCont = EncodedCont.apply(lambda x: OneHotEncodeSingleCont(x), axis=1)\n",
    "\n",
    "# Drop the original \"continent\" column as it is no longer needed\n",
    "EncodedCont = EncodedCont.drop(columns=[\"continent\"])\n",
    "\n",
    "# Display the one-hot encoded continent DataFrame for verification\n",
    "print(\"One-Hot Encoded Continent DataFrame:\")\n",
    "print(EncodedCont.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5: Combining and Scaling Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "EncodedCont = EncodedCont.reindex(Train.index)\n",
    "ratings = ratings.reindex(Train.index)\n",
    "TitleEmbeddings = TitleEmbeddings.reindex(Train.index)\n",
    "KeyWordEmbeddings = KeyWordEmbeddings.reindex(Train.index)\n",
    "GenreEmbeddings = GenreEmbeddings.reindex(Train.index)\n",
    "EncodedGenres = EncodedGenres.reindex(Train.index)\n",
    "\n",
    "numericColumnSet = [\n",
    "    'num_critic_for_reviews',\n",
    "    'duration',\n",
    "    'director_facebook_likes',\n",
    "    'actor_3_facebook_likes',\n",
    "    'actor_1_facebook_likes',\n",
    "    'gross',\n",
    "    'num_voted_users',\n",
    "    'cast_total_facebook_likes',\n",
    "    'facenumber_in_poster',\n",
    "    'num_user_for_reviews',\n",
    "    'title_year',\n",
    "    'actor_2_facebook_likes',\n",
    "    'movie_facebook_likes',\n",
    "    'average_degree_centrality'\n",
    "]\n",
    "\n",
    "\n",
    "# PreTotalData = numeric + continents + ratings\n",
    "PreTotalData = pd.concat([Train[numericColumnSet], EncodedCont, ratings], axis=1)\n",
    "PreTotalData.columns = PreTotalData.columns.astype(str)  # ensure column names are strings\n",
    "\n",
    "# TotalData = numeric + continents + ratings + TitleEmbeddings + KeyWordEmbeddings\n",
    "TotalData = pd.concat(\n",
    "    [Train[numericColumnSet], EncodedCont, ratings, TitleEmbeddings, KeyWordEmbeddings],\n",
    "    axis=1\n",
    ")\n",
    "TotalData.columns = TotalData.columns.astype(str)\n",
    "\n",
    "# TotalDataPlusGenreEmbeddings = TotalData + GenreEmbeddings\n",
    "TotalDataPlusGenreEmbeddings = pd.concat([TotalData, GenreEmbeddings], axis=1)\n",
    "TotalDataPlusGenreEmbeddings.columns = TotalDataPlusGenreEmbeddings.columns.astype(str)\n",
    "\n",
    "# TotalDataPlusGenreEncodings = TotalData + EncodedGenres\n",
    "TotalDataPlusGenreEncodings = pd.concat([TotalData, EncodedGenres], axis=1)\n",
    "TotalDataPlusGenreEncodings.columns = TotalDataPlusGenreEncodings.columns.astype(str)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale PreTotalData\n",
    "S_TotalData_array = scaler.fit_transform(PreTotalData)\n",
    "S_TotalData = pd.DataFrame(S_TotalData_array, columns=PreTotalData.columns)\n",
    "\n",
    "# Scale TotalDataPlusGenreEmbeddings\n",
    "scaled_embeddings_data = scaler.fit_transform(TotalDataPlusGenreEmbeddings)\n",
    "S_TotalDataPlusGenreEmbeddings = pd.DataFrame(\n",
    "    scaled_embeddings_data,\n",
    "    columns=TotalDataPlusGenreEmbeddings.columns\n",
    ")\n",
    "\n",
    "# Scale TotalDataPlusGenreEncodings\n",
    "scaled_encoding_data = scaler.fit_transform(TotalDataPlusGenreEncodings)\n",
    "S_TotalDataPlusGenreEncoding = pd.DataFrame(\n",
    "    scaled_encoding_data,\n",
    "    columns=TotalDataPlusGenreEncodings.columns\n",
    ")\n",
    "\n",
    "# Verify Shapes\n",
    "print(\"Shape of S_TotalData:\", S_TotalData.shape)\n",
    "print(\"Shape of S_TotalDataPlusGenreEmbeddings (scaled):\", S_TotalDataPlusGenreEmbeddings.shape)\n",
    "print(\"Shape of S_TotalDataPlusGenreEncoding (scaled):\", S_TotalDataPlusGenreEncoding.shape)\n",
    "\n",
    "\n",
    "y = Train[\"imdb_score_binned\"]  # the target\n",
    "\n",
    "# Split for S_TotalData\n",
    "X_train_S_TotalData, X_test_S_TotalData, y_train_S_TotalData, y_test_S_TotalData = train_test_split(\n",
    "    S_TotalData,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Split for S_TotalDataPlusGenreEmbeddings\n",
    "X_train_S_TotalDataPlusGenreEmbeddings, X_test_S_TotalDataPlusGenreEmbeddings, y_train_S_TotalDataPlusGenreEmbeddings, y_test_S_TotalDataPlusGenreEmbeddings = train_test_split(\n",
    "    S_TotalDataPlusGenreEmbeddings,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Split for S_TotalDataPlusGenreEncoding\n",
    "X_train_S_TotalDataPlusGenreEncoding, X_test_S_TotalDataPlusGenreEncoding, y_train_S_TotalDataPlusGenreEncoding, y_test_S_TotalDataPlusGenreEncoding = train_test_split(\n",
    "    S_TotalDataPlusGenreEncoding,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"S_TotalData train shape:\", X_train_S_TotalData.shape, \"test shape:\", X_test_S_TotalData.shape)\n",
    "print(\"S_TotalDataPlusGenreEmbeddings train shape:\", X_train_S_TotalDataPlusGenreEmbeddings.shape,\n",
    "      \"test shape:\", X_test_S_TotalDataPlusGenreEmbeddings.shape)\n",
    "print(\"S_TotalDataPlusGenreEncoding train shape:\", X_train_S_TotalDataPlusGenreEncoding.shape,\n",
    "      \"test shape:\", X_test_S_TotalDataPlusGenreEncoding.shape)\n",
    "\n",
    "# Check data types if desired\n",
    "print(\"\\nType of X_train_S_TotalDataPlusGenreEncoding:\", type(X_train_S_TotalDataPlusGenreEncoding))\n",
    "print(\"Type of y_train_S_TotalDataPlusGenreEncoding:\", type(y_train_S_TotalDataPlusGenreEncoding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Feature Engineering and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: Principal Component Analysis (PCA) on Keyword Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Perform PCA on the KeyWordEmbeddings\n",
    "pca = PCA()\n",
    "pca.fit(KeyWordEmbeddings)  # Fit PCA model on the embeddings\n",
    "\n",
    "# Extract explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Set up the plot aesthetics using seaborn\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create the figure and axis objects for cumulative variance plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot cumulative variance with markers\n",
    "ax.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, \n",
    "        marker='o', linestyle='--', label='Cumulative Explained Variance', color='blue')\n",
    "\n",
    "# Highlight the point where 95% variance is explained\n",
    "components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "ax.axhline(y=0.95, color='red', linestyle='-', label='95% Variance Threshold')\n",
    "ax.axvline(x=components_95, color='green', linestyle='--', \n",
    "           label=f'{components_95} Components for 95% Variance')\n",
    "\n",
    "# Add annotations for clarity\n",
    "ax.annotate(f'{components_95} Components', \n",
    "            xy=(components_95, 0.95), \n",
    "            xytext=(components_95 + 5, 0.85),\n",
    "            arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "            fontsize=12)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Explained Variance vs. Number of Principal Components', fontsize=16)\n",
    "ax.set_xlabel('Number of Principal Components', fontsize=14)\n",
    "ax.set_ylabel('Cumulative Explained Variance', fontsize=14)\n",
    "ax.set_xlim(1, len(cumulative_variance))\n",
    "ax.set_ylim(0.0, 1.05)\n",
    "ax.legend(loc='lower right', fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Heatmap to visualize contribution of principal components\n",
    "contribution_matrix = pca.components_[:components_95]\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(contribution_matrix, cmap='coolwarm', center=0, \n",
    "            annot=False, fmt=\".2f\", xticklabels=False, yticklabels=True)\n",
    "plt.title(f'Contribution of First {components_95} Principal Components', fontsize=16)\n",
    "plt.xlabel('Features', fontsize=14)\n",
    "plt.ylabel('Principal Components', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature Evaluation and PCA on Genre Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate mutual information between title embeddings and binned IMDb scores\n",
    "print(\"Mutual Information between Title Embeddings and IMDb Score Bins:\")\n",
    "mi_scores = mutual_info_classif(TitleEmbeddings, Train[\"imdb_score_binned\"])\n",
    "print(mi_scores)\n",
    "\n",
    "# Calculate mutual information scores\n",
    "mi_scores = mutual_info_classif(TitleEmbeddings, Train[\"imdb_score_binned\"])\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "mi_df = pd.DataFrame({'Feature': [f'Feature_{i+1}' for i in range(len(mi_scores))], 'MI Score': mi_scores})\n",
    "\n",
    "# Sort the DataFrame by MI scores in descending order for better insights\n",
    "mi_df = mi_df.sort_values(by='MI Score', ascending=False)\n",
    "\n",
    "# Plot the mutual information scores as a bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(mi_df['Feature'], mi_df['MI Score'], color='skyblue')\n",
    "\n",
    "# Add plot labels and title\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Mutual Information Score', fontsize=12)\n",
    "plt.title('Mutual Information Scores for Title Embeddings', fontsize=14)\n",
    "\n",
    "# Rotate x-axis labels for better visibility\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('\\n\\n\\n')\n",
    "\n",
    "# Perform PCA on GenreEmbeddings\n",
    "pca = PCA()\n",
    "pca.fit(GenreEmbeddings)\n",
    "\n",
    "# Print the explained variance ratio for PCA\n",
    "print(\"Explained Variance Ratio for Genre Embeddings PCA:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot cumulative explained variance for GenreEmbeddings\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)  # Set the figure size\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# x-axis: Component indices (1-based for human readability)\n",
    "xi = np.arange(1, len(pca.explained_variance_ratio_) + 1, step=1)\n",
    "\n",
    "# y-axis: Cumulative sum of explained variance ratios\n",
    "y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Plot cumulative explained variance\n",
    "plt.ylim(0.0, 1.1)  # Set y-axis range\n",
    "plt.plot(xi, y, marker='o', linestyle='--', color='b', label='Cumulative Variance')\n",
    "\n",
    "# Add plot labels and title\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(0, len(pca.explained_variance_ratio_) + 1, step=10))  # Adjust step size if needed\n",
    "plt.ylabel('Cumulative Variance (%)')\n",
    "plt.title('The Number of Components Needed to Explain Variance')\n",
    "\n",
    "# Add reference line for the 95% variance threshold\n",
    "plt.axhline(y=0.95, color='r', linestyle='-', label='95% Threshold')\n",
    "plt.text(10, 0.88, '95% cut-off threshold', color='red', fontsize=12)\n",
    "\n",
    "# Add gridlines and legend for clarity\n",
    "ax.grid(axis='x')\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3: Partial Least Squares (PLS) Regression and Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features and target variable\n",
    "X_scaled = S_TotalDataPlusGenreEmbeddings\n",
    "y_scaled = Train[\"imdb_score_binned\"].values.reshape(-1, 1).ravel()\n",
    "\n",
    "# Fit PLS regression with standardized data\n",
    "pls = PLSRegression(n_components=len(S_TotalDataPlusGenreEmbeddings.columns) - 1, max_iter=100000)\n",
    "pls.fit(X_scaled, y_scaled)\n",
    "\n",
    "# Evaluate R² for individual components and overall performance\n",
    "def pls_explained_variance(pls, X, Y_true, do_plot=True):\n",
    "    r2 = np.zeros(pls.n_components)\n",
    "    x_transformed = pls.transform(X)\n",
    "    for i in range(0, pls.n_components):\n",
    "        Y_pred = (np.dot(x_transformed[:, i][:, np.newaxis],\n",
    "                         pls.y_loadings_[:, i][:, np.newaxis].T) * pls._y_std\n",
    "                  + pls._y_mean)\n",
    "        r2[i] = r2_score(Y_true, Y_pred)\n",
    "    \n",
    "    overall_r2 = r2_score(Y_true, pls.predict(X))\n",
    "\n",
    "    if do_plot:\n",
    "        import seaborn as sns\n",
    "        sns.set(style=\"whitegrid\")\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.lineplot(x=np.arange(1, pls.n_components + 1), y=r2, marker='o', label='Individual R²')\n",
    "        plt.axhline(overall_r2, color='r', linestyle='--', label=f'Overall R²: {overall_r2:.3f}')\n",
    "        plt.xlabel('PLS Component #', fontsize=12)\n",
    "        plt.ylabel('R²', fontsize=12)\n",
    "        plt.title(f'Summed Individual R²: {np.sum(r2):.3f}, Overall R²: {overall_r2:.3f}', fontsize=14)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return r2, overall_r2\n",
    "\n",
    "# Evaluate and plot explained variance\n",
    "r2, overall_r2 = pls_explained_variance(pls, X_scaled, y_scaled, do_plot=True)\n",
    "\n",
    "# Find the optimal number of PLS components using cross-validation\n",
    "best_r2 = float('-inf')\n",
    "best_n_components = 0\n",
    "\n",
    "for n in range(1, 40):  # Test components from 1 to 20\n",
    "    pls = PLSRegression(n_components=n)\n",
    "    scores = cross_val_score(pls, X_scaled, y_scaled, cv=5, scoring='r2')\n",
    "    mean_r2 = np.mean(scores)\n",
    "    # print(f\"Components: {n}, Mean R²: {mean_r2:.3f}\")\n",
    "    if mean_r2 > best_r2:\n",
    "        best_r2 = mean_r2\n",
    "        best_n_components = n\n",
    "\n",
    "print(f\"Best number of components: {best_n_components}, Best R²: {best_r2:.3f}\")\n",
    "\n",
    "# Fit PLS with the optimal number of components\n",
    "pls = PLSRegression(n_components=best_n_components)\n",
    "pls.fit(X_scaled, y_scaled)\n",
    "r2, overall_r2 = pls_explained_variance(pls, X_scaled, y_scaled, do_plot=True)\n",
    "\n",
    "\n",
    "# Calculate cumulative variance for selected components\n",
    "PLScomponents = pls.transform(X_scaled)\n",
    "tempList = []\n",
    "for i in range(1, best_n_components + 1):\n",
    "    Itemp = PLScomponents[:, i - 1]\n",
    "    Jtemp = y_scaled\n",
    "    tempList.append(np.abs(np.cov(Itemp, Jtemp)[1, 0]))\n",
    "\n",
    "# Plot cumulative variance explained by components\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "xi = np.arange(1, best_n_components + 1, step=1)\n",
    "y = np.cumsum(tempList) / sum(tempList)\n",
    "\n",
    "# Calculate cumulative variance for selected components\n",
    "PLScomponents = pls.transform(X_scaled)\n",
    "tempList = []\n",
    "for i in range(1, best_n_components + 1):\n",
    "    Itemp = PLScomponents[:, i - 1]\n",
    "    Jtemp = y_scaled\n",
    "    tempList.append(np.abs(np.cov(Itemp, Jtemp)[1, 0]))\n",
    "\n",
    "# Plot cumulative variance explained by components\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "xi = np.arange(1, best_n_components + 1, step=1)\n",
    "y = np.cumsum(tempList) / sum(tempList)\n",
    "\n",
    "n_comp_95 = np.argmax(y >= 0.95) + 1\n",
    "\n",
    "# Existing plot code\n",
    "sns.lineplot(x=xi, y=y, marker='o', linestyle='--', color='b', label='Cumulative Variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='-', label='95% Cut-off Threshold')\n",
    "\n",
    "# Existing vertical line/text for best_n_components\n",
    "plt.vlines(best_n_components, 0, 1.2, linestyles='dotted', colors='gray', \n",
    "           label=f'Selected Components ({best_n_components})')\n",
    "plt.text(best_n_components + 0.5, 0.5, f'{best_n_components} Best R² Components', \n",
    "         color='black', fontsize=10)\n",
    "\n",
    "plt.vlines(n_comp_95, 0, 1.2, linestyles='--', colors='green',\n",
    "           label=f'95% Variance at {n_comp_95} Components')\n",
    "plt.text(n_comp_95 + 0.5, 0.4, f'{n_comp_95} Components for 95% Var',\n",
    "         color='green', fontsize=10)\n",
    "\n",
    "plt.ylim(0.0, 1.1)\n",
    "plt.xlabel('Number of Components', fontsize=12)\n",
    "plt.ylabel('Cumulative Variance (%)', fontsize=12)\n",
    "plt.title('Cumulative Variance Explained by Selected Components', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Dataset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Baseline Whole Dataset Evaluation with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetWrapper:\n",
    "    def __init__(self, name, X_train, y_train, X_test, y_test):\n",
    "        self.name = name\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "# Create your dataset wrappers (replace with your own variables)\n",
    "datasets = [\n",
    "    DatasetWrapper(\n",
    "        \"TotalData\",\n",
    "        X_train_S_TotalData,\n",
    "        y_train_S_TotalData,\n",
    "        X_test_S_TotalData,\n",
    "        y_test_S_TotalData\n",
    "    ),\n",
    "    DatasetWrapper(\n",
    "        \"TotalDataPlusGenreEncodings\",\n",
    "        X_train_S_TotalDataPlusGenreEncoding,\n",
    "        y_train_S_TotalDataPlusGenreEncoding,\n",
    "        X_test_S_TotalDataPlusGenreEncoding,\n",
    "        y_test_S_TotalDataPlusGenreEncoding\n",
    "    ),\n",
    "    DatasetWrapper(\n",
    "        \"TotalDataPlusGenreEmbeddings\",\n",
    "        X_train_S_TotalDataPlusGenreEmbeddings,\n",
    "        y_train_S_TotalDataPlusGenreEmbeddings,\n",
    "        X_test_S_TotalDataPlusGenreEmbeddings,\n",
    "        y_test_S_TotalDataPlusGenreEmbeddings\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define the models to evaluate\n",
    "models = [\n",
    "    (\"Logistic Regression\", LogisticRegression(max_iter=100000)),\n",
    "    (\"Support Vector Classifier (SVC)\", SVC())\n",
    "]\n",
    "\n",
    "\n",
    "# Function to evaluate models on multiple datasets with cross-validation\n",
    "def evaluate_models_on_splits(datasets, models):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models on multiple train-test splits.\n",
    "\n",
    "    Parameters:\n",
    "    - datasets: List of DatasetWrapper objects, each containing X_train, y_train, X_test, y_test\n",
    "    - models: List of (model_name, model_instance) tuples\n",
    "\n",
    "    Prints out the accuracy on the test set for each dataset and each model.\n",
    "    \"\"\"\n",
    "    for ds in datasets:\n",
    "        print(f\"\\nEvaluating Dataset: {ds.name}\")\n",
    "        for model_name, model in models:\n",
    "            # Fit the model on the train set\n",
    "            model.fit(ds.X_train, ds.y_train)\n",
    "            # Predict on the test set\n",
    "            y_pred = model.predict(ds.X_test)\n",
    "            # Calculate test accuracy\n",
    "            accuracy = accuracy_score(ds.y_test, y_pred)\n",
    "            print(f\"  Model: {model_name} | Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_models_on_splits(datasets, models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Selection Using Ablation Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_SVM(X, Y, Xt, Yt):\n",
    "    \"\"\"\n",
    "    Trains an SVM on (X, Y) and returns accuracy on (Xt, Yt).\n",
    "    \"\"\"\n",
    "    clf = SVC()\n",
    "    clf.fit(X, Y)\n",
    "    return clf.score(Xt, Yt)\n",
    "\n",
    "def accuracy_Logistic(X, Y, Xt, Yt):\n",
    "    \"\"\"\n",
    "    Trains a Logistic Regression on (X, Y) and returns accuracy on (Xt, Yt).\n",
    "    \"\"\"\n",
    "    clf = LogisticRegression(max_iter=100000, random_state=1273213)\n",
    "    clf.fit(X, Y)\n",
    "    return clf.score(Xt, Yt)\n",
    "\n",
    "\n",
    "def ablationFeatureSelection(fullDataSet, \n",
    "                             targetSeries, \n",
    "                             scoreFunc, \n",
    "                             MaxOrMin, \n",
    "                             nSplits, \n",
    "                             threshold=False):\n",
    "    \"\"\"\n",
    "    Given a dataset (fullDataSet) and a target (targetSeries),\n",
    "    performs 1-step 'ablation': \n",
    "      1) Compute the baseline score across K-folds \n",
    "      2) For each feature, drop it and measure the new score\n",
    "      3) Return the feature whose removal yields the \"best\" score \n",
    "         (depending on MaxOrMin), along with that score.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=nSplits, random_state=10201331, shuffle=True)\n",
    "\n",
    "    # 1) Compute baseline\n",
    "    baseline_scores = []\n",
    "    for train_index, test_index in kf.split(fullDataSet):\n",
    "        X_train = fullDataSet.iloc[train_index]\n",
    "        Y_train = targetSeries.iloc[train_index]\n",
    "        X_test  = fullDataSet.iloc[test_index]\n",
    "        Y_test  = targetSeries.iloc[test_index]\n",
    "        baseline_scores.append(scoreFunc(X_train, Y_train, X_test, Y_test))\n",
    "    baseline = stat.mean(baseline_scores)\n",
    "    print(\"Ablation baseline score:\", baseline)\n",
    "\n",
    "    # 2) Check the score for removing each feature\n",
    "    feature_scores = {}\n",
    "    for col in fullDataSet.columns:\n",
    "        tmpData = fullDataSet.drop(columns=[col], axis=1)\n",
    "        tmp_scores = []\n",
    "        for train_index, test_index in kf.split(tmpData):\n",
    "            X_train = tmpData.iloc[train_index]\n",
    "            Y_train = targetSeries.iloc[train_index]\n",
    "            X_test  = tmpData.iloc[test_index]\n",
    "            Y_test  = targetSeries.iloc[test_index]\n",
    "            tmp_scores.append(scoreFunc(X_train, Y_train, X_test, Y_test))\n",
    "        feature_scores[col] = stat.mean(tmp_scores)\n",
    "\n",
    "    # 3) Return the column that yields the \"best\" (max or min) score\n",
    "    if MaxOrMin == 'maxamise':\n",
    "        best_val = max(feature_scores.values())\n",
    "        best_cols = [c for c, val in feature_scores.items() if val == best_val]\n",
    "    else:\n",
    "        best_val = min(feature_scores.values())\n",
    "        best_cols = [c for c, val in feature_scores.items() if val == best_val]\n",
    "\n",
    "    chosen_col = random.choice(best_cols)  # pick one if there's a tie\n",
    "    return chosen_col, feature_scores[chosen_col]\n",
    "\n",
    "\n",
    "def buildUp(currFeatureSet, \n",
    "            fullDataSet, \n",
    "            targetSeries, \n",
    "            scoreFunc, \n",
    "            MaxOrMin, \n",
    "            nSplits, \n",
    "            threshold=False):\n",
    "    \"\"\"\n",
    "    Attempt to 'build up' features one-by-one:\n",
    "      - Start from currFeatureSet (possibly empty)\n",
    "      - For each column not yet in currFeatureSet, \n",
    "        add it and compute the cross-validation score\n",
    "      - Return the column that yields the best improvement.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=nSplits, random_state=10201331, shuffle=True)\n",
    "    \n",
    "    # Evaluate each feature *not* in currFeatureSet\n",
    "    candidate_scores = {}\n",
    "    for col in fullDataSet.columns:\n",
    "        if col in currFeatureSet:\n",
    "            continue\n",
    "        combo = currFeatureSet + [col]\n",
    "        tmp_scores = []\n",
    "        for train_index, test_index in kf.split(fullDataSet):\n",
    "            X_train = fullDataSet[combo].iloc[train_index]\n",
    "            Y_train = targetSeries.iloc[train_index]\n",
    "            X_test  = fullDataSet[combo].iloc[test_index]\n",
    "            Y_test  = targetSeries.iloc[test_index]\n",
    "            tmp_scores.append(scoreFunc(X_train, Y_train, X_test, Y_test))\n",
    "        candidate_scores[col] = stat.mean(tmp_scores)\n",
    "\n",
    "    # If there are no candidates left, return None\n",
    "    if len(candidate_scores) == 0:\n",
    "        return None, None\n",
    "\n",
    "    # Identify the best feature to add\n",
    "    if MaxOrMin == 'maxamise':\n",
    "        best_val = max(candidate_scores.values())\n",
    "        best_cols = [c for c, val in candidate_scores.items() if val == best_val]\n",
    "    else:\n",
    "        best_val = min(candidate_scores.values())\n",
    "        best_cols = [c for c, val in candidate_scores.items() if val == best_val]\n",
    "\n",
    "    chosen_col = random.choice(best_cols)\n",
    "    return chosen_col, candidate_scores[chosen_col]\n",
    "\n",
    "random.seed = 10023913  # Proper usage of random seed\n",
    "\n",
    "ablationAttempt = S_TotalDataPlusGenreEncoding.copy(deep=True)\n",
    "currFeat = []\n",
    "baseLineAccuracy = 0\n",
    "\n",
    "# BUILD UP STAGE\n",
    "while True:\n",
    "    columnName, accuracy = buildUp(currFeat,\n",
    "                                   ablationAttempt,\n",
    "                                   Train[\"imdb_score_binned\"],\n",
    "                                   accuracy_Logistic,\n",
    "                                   'maxamise',\n",
    "                                   5)\n",
    "    if (columnName is None) or (accuracy is None):\n",
    "        # No more features to add\n",
    "        break\n",
    "    if accuracy < baseLineAccuracy:\n",
    "        # Stopping if there's no improvement\n",
    "        break\n",
    "    print(f\"Adding feature: {columnName}, new accuracy = {accuracy}\")\n",
    "    currFeat.append(columnName)\n",
    "    baseLineAccuracy = accuracy\n",
    "    print(\"Current combo:\", currFeat)\n",
    "\n",
    "# ABLATION STAGE\n",
    "baseline = baseLineAccuracy\n",
    "while len(ablationAttempt[currFeat].columns) > 1:\n",
    "    print(\"Ablation with feature set size:\", len(currFeat))\n",
    "    columnName, newScore = ablationFeatureSelection(\n",
    "        ablationAttempt[currFeat], \n",
    "        Train[\"imdb_score_binned\"], \n",
    "        accuracy_Logistic,\n",
    "        'maxamise',\n",
    "        5\n",
    "    )\n",
    "    if newScore < baseline:\n",
    "        print(f\"No improvement after trying to remove '{columnName}' (score {newScore} < baseline {baseline}). Stopping.\")\n",
    "        break\n",
    "    print(f\"Removing column: {columnName}, new ablation score = {newScore}\")\n",
    "    currFeat.remove(columnName)\n",
    "    baseline = newScore\n",
    "\n",
    "print(\"Final selected features:\", currFeat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluation of Model on PLS Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=20, random_state=10201331, shuffle=True)\n",
    "\n",
    "avgScr = []\n",
    "for train_index, test_index in kf.split(PLScomponents):\n",
    "    # Slice the first n_comp_95 components for train and test\n",
    "    X = PLScomponents[train_index, :n_comp_95]\n",
    "    Y = Train[\"imdb_score_binned\"].iloc[train_index]\n",
    "    Xt = PLScomponents[test_index, :n_comp_95]\n",
    "    Yt = Train[\"imdb_score_binned\"].iloc[test_index]\n",
    "    \n",
    "    # Fit the Logistic Regression model\n",
    "    clf = LogisticRegression(max_iter=100000)\n",
    "    clf.fit(X, Y)\n",
    "    \n",
    "    # Evaluate on the test fold\n",
    "    avgScr.append(clf.score(Xt, Yt))\n",
    "\n",
    "# Print the mean accuracy across the 20 folds\n",
    "print(\"Mean CV Accuracy:\", stat.mean(avgScr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Model Optimisation and Evaluation with Ablation Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = ['num_voted_users', 'Drama', 'title_year', 'gross', 'Action', 'Documentary', 'KWE_61', 'Asia', 'Animation', 'KWE_57', 'Musical', 'TitleEmb_10', 'KWE_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_S_TotalDataPlusGenreEncoding[final_features]\n",
    "y_train = y_train_S_TotalDataPlusGenreEncoding\n",
    "\n",
    "X_test = X_test_S_TotalDataPlusGenreEncoding[final_features]\n",
    "y_test = y_test_S_TotalDataPlusGenreEncoding\n",
    "\n",
    "# parameter grid\n",
    "param_grid = [\n",
    "    {\n",
    "        'penalty': ['l1'],\n",
    "        'solver': ['liblinear', 'saga'],  # only solvers that support l1\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'class_weight': [None]\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['liblinear', 'lbfgs', 'sag', 'saga'],  # solvers that support l2\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'class_weight': [None]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(random_state=1273213, max_iter=100000)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',   # or 'f1_macro', 'balanced_accuracy', etc.\n",
    "    cv=5,                 # 5-fold cross-validation\n",
    "    n_jobs=-1,            # use all available cores\n",
    "    verbose=1             # print progress\n",
    ")\n",
    "\n",
    "# Fit the grid search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearch\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV accuracy:\", grid_search.best_score_)\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_  # best estimator from grid search\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # set figure size\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\", fontsize=16)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "\n",
    "# Adjust tick label size (optional)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example param grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],      # Number of trees\n",
    "    'max_depth': [None, 5, 10],          # Tree depth\n",
    "    'max_features': ['sqrt', 'log2'],    # Features to consider at each split\n",
    "    'min_samples_split': [2, 5],         # Min # of samples to split a node\n",
    "    'min_samples_leaf': [1, 2, 4],       # Min # of samples at a leaf node\n",
    "    'class_weight': [None, 'balanced'],  # Class imbalance handling\n",
    "}\n",
    "\n",
    "# Create a RandomForestClassifier (baseline model)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Set up the grid search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',   # or 'f1_macro', etc.\n",
    "    cv=5,                 # 5-fold cross-validation\n",
    "    n_jobs=-1,            # use all available CPU cores\n",
    "    verbose=1             # print progress messages\n",
    ")\n",
    "\n",
    "# Fit on the training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print out the best hyperparameters from the grid search\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best CV accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Retrieve the best model from the grid search\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))  # set figure size\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\", fontsize=16)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "\n",
    "# Adjust tick label size (optional)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1) Prepare X_train, y_train, X_test, y_test\n",
    "#    (Already created by your train-test split, for instance 80/20)\n",
    "#    We assume these variables exist in your workspace.\n",
    "# -------------------------------------------------------------------------\n",
    "# X_train, y_train, X_test, y_test = ...\n",
    "\n",
    "# If you have an extremely imbalanced dataset and your target is binary,\n",
    "# it might be beneficial to look at 'f1', 'recall', or 'balanced_accuracy' \n",
    "# for scoring, in addition to (or instead of) accuracy.\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2) Define a parameter grid. Note scale_pos_weight for imbalance\n",
    "# -------------------------------------------------------------------------\n",
    "param_grid = {\n",
    "    'n_estimators':      [100, 200, 300],\n",
    "    'learning_rate':     [0.01, 0.1, 0.2],\n",
    "    'max_depth':         [3, 5, 7],\n",
    "    'subsample':         [0.8, 1.0],\n",
    "    'colsample_bytree':  [0.8, 1.0],\n",
    "    'min_child_weight':  [1, 3],\n",
    "    'gamma':             [0, 1],\n",
    "    # scale_pos_weight: number_of_negative_examples / number_of_positive_examples\n",
    "    # If unsure, you can try a range of plausible values.\n",
    "    'scale_pos_weight':  [1, 5, 10]\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3) Create and configure the XGBClassifier\n",
    "#    - objective='binary:logistic' for binary classification\n",
    "#    - use_label_encoder=False to avoid label-encoding warnings\n",
    "#    - eval_metric='logloss' or 'auc' internally for xgboost\n",
    "# -------------------------------------------------------------------------\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',  \n",
    "    eval_metric='logloss',        \n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4) Set up the GridSearchCV\n",
    "#    - Consider an appropriate scoring metric for imbalanced data\n",
    "#      (e.g. 'f1', 'balanced_accuracy', 'roc_auc', etc.)\n",
    "# -------------------------------------------------------------------------\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',     # or 'f1_macro', 'balanced_accuracy', 'roc_auc'\n",
    "    cv=5,                   # 5-fold cross-validation\n",
    "    n_jobs=-1,              # utilize all cores\n",
    "    verbose=1               # print progress\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 5) Fit the grid search on the training data\n",
    "# -------------------------------------------------------------------------\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best hyperparameters and CV score\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best CV score:\", grid_search.best_score_)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 6) Evaluate on the held-out test set\n",
    "# -------------------------------------------------------------------------\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# Calculate test accuracy (or use an alternative metric)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nTest Accuracy:\", test_acc)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"XGBoost - Confusion Matrix\", fontsize=16)\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12)\n",
    "plt.ylabel(\"True Label\", fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as dataUtils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, nInputs, l1, l2, l3, nClasses):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.Numerical = torch.nn.Sequential(\n",
    "            torch.nn.Linear(20, 10),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(10,10),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.Embeddings1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(100, 20),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(20,10),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.Embeddings2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(100,20),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(20,10),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.Embeddings3 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(100,20),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(20,10),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Dropout(p=0.25)\n",
    "        )\n",
    "        self.Combination = torch.nn.Sequential(\n",
    "            torch.nn.Linear(40,10),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Dropout(p=0.25),\n",
    "            torch.nn.Linear(10,nClasses),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        a = self.Numerical(x[:,0:20])\n",
    "        b = self.Embeddings1(x[:,20:120])\n",
    "        c = self.Embeddings2(x[:,120:220])\n",
    "        d = self.Embeddings3(x[:,220:320])\n",
    "        combined = torch.cat((a,b,c,d), dim=1)\n",
    "        out = self.Combination(combined)\n",
    "        return out\n",
    "\n",
    "def test(model, criterion, test_loader):\n",
    "    test_loss = 0.\n",
    "    test_preds, test_labels = list(), list()\n",
    "    for i, data in enumerate(test_loader):\n",
    "        x, labels = data\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            test_loss += criterion(input=logits, target=labels).item()\n",
    "            test_preds.append(predictions)\n",
    "            test_labels.append(labels)\n",
    "\n",
    "    test_preds = torch.cat(test_preds)\n",
    "    test_labels = torch.cat(test_labels)\n",
    "\n",
    "    test_accuracy = torch.eq(test_preds, test_labels).float().mean().item()\n",
    "\n",
    "    print('[TEST] Mean loss {:.4f} | Accuracy {:.4f}'.format(test_loss/len(test_loader), test_accuracy))\n",
    "\n",
    "\n",
    "def train(model, train_loader, test_loader, optimizer, n_epochs=10):\n",
    "    LOG_INTERVAL = 250\n",
    "    running_loss, running_accuracy = list(), list()\n",
    "    start_time = time.time()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        epoch_loss = 0.\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "\n",
    "            x, labels = data\n",
    "\n",
    "            logits = model(x)\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            train_acc = torch.mean(torch.eq(predictions, labels).float()).item()\n",
    "\n",
    "            loss = criterion(input=logits, target=labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "            running_accuracy.append(train_acc)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if i % LOG_INTERVAL == 0:\n",
    "                deltaT = time.time() - start_time\n",
    "                mean_loss = epoch_loss / (i+1)\n",
    "                print('[TRAIN] Epoch {} [{}/{}]| Mean loss {:.4f} | Train accuracy {:.5f} | Time {:.2f} s'.format(epoch, \n",
    "                    i, len(train_loader), mean_loss, train_acc, deltaT))\n",
    "\n",
    "        print('Epoch complete! Mean loss: {:.4f}'.format(epoch_loss/len(train_loader)))\n",
    "\n",
    "        test(model, criterion, test_loader)\n",
    "    return running_loss, running_accuracy\n",
    "\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(TotalDataPlusGenreEmbeddings, Train[\"imdb_score_binned\"], test_size=0.2, random_state=981488)\n",
    "\n",
    "xTrain = torch.tensor(xTrain.values, dtype=torch.float)\n",
    "yTrain = torch.tensor(yTrain.values).type(torch.LongTensor)\n",
    "trainData = dataUtils.TensorDataset(xTrain, yTrain)\n",
    "trainLoader = dataUtils.DataLoader(trainData, batch_size=128, shuffle=True)\n",
    "\n",
    "xTest = torch.tensor(xTest.values, dtype=torch.float)\n",
    "yTest = torch.tensor(yTest.values).type(torch.LongTensor)\n",
    "testData = dataUtils.TensorDataset(xTest, yTest)\n",
    "testLoader = dataUtils.DataLoader(testData, batch_size=128, shuffle=False)\n",
    "\n",
    "neuralNetwork = NeuralNetwork(len(TotalDataPlusGenreEmbeddings.columns), 150, 100, 50, 5)\n",
    "\n",
    "optimizer = torch.optim.SGD(neuralNetwork.parameters(), lr=1e-2, momentum=0.5)\n",
    "\n",
    "nnLoss, nnAcc = train(neuralNetwork, trainLoader, testLoader, optimizer, 100)\n",
    "\n",
    "plt.plot(nnLoss)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Cross-entropy Loss (Train)\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(nnAcc)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Accuracy (Train)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
